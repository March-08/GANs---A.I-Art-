{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NumberGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2WaNhBDwRwTG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k5mFBuzzl2a",
        "outputId": "1faed537-6d5c-4919-c522-2851209a0780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "!mkdir generated_images"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘generated_images’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Variables for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RThZMDruz9cB",
        "outputId": "13bc2748-f55b-4f13-9f22-e3b0c5d895c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_width = 28\n",
        "img_height = 28\n",
        "channels = 1\n",
        "img_shape = (img_width, img_height)\n",
        "latent_dim  = 100 #latent = noise \n",
        "adam = Adam(lr = 0.0001)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiqZpri0iQh",
        "outputId": "65610a2a-05d9-4bd6-f55f-7c5f288088d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#takes in input the noise (latent) and constructs the image\n",
        "def build_generator():\n",
        "  model = Sequential([\n",
        "    Dense(256, input_dim = latent_dim),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "    BatchNormalization(momentum = 0.8),\n",
        "\n",
        "    Dense(512),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "    BatchNormalization(momentum = 0.8),\n",
        "\n",
        "    Dense(1024),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "    BatchNormalization(momentum = 0.8),\n",
        "\n",
        "    #in GANs activation in always tanh\n",
        "    #we need to create an output shape that matches our images\n",
        "    Dense(np.prod(img_shape), activation = \"tanh\"),\n",
        "    Reshape(img_shape)\n",
        "  ])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 784)               803600    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 28, 28)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2JzEAPv0lKt",
        "outputId": "38c019dc-3d4b-4ccc-baf1-3db949cc1978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_discriminator():\n",
        "\n",
        "  model = Sequential([\n",
        "    Flatten(input_shape = img_shape),\n",
        "    Dense(512),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "    Dense(256),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "    #one neuron because classifies \"fake\" or \"real\"\n",
        "    Dense(1, activation= \"sigmoid\"),\n",
        "  ])\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ue3TEd0xLy",
        "outputId": "b5b6756d-a7de-436e-dbae-4c1b33c4b033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#in oue GAN we don't train discriminator\n",
        "GAN = Sequential([\n",
        "   generator,\n",
        "   discriminator             \n",
        "])\n",
        "\n",
        "#make discriminator not trainable in GAN\n",
        "GAN.layers[1].trainable = False\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "GAN.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 28, 28)            1493520   \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 1)                 533505    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,027,025\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 537,089\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    #generate 25 images to fit on a 5 x 5 grid for our animation!\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "    #print(\"%.8f\" % save_name)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[ :,:,0], cmap='gray')\n",
        "            #axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            # axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    #print('saved')\n",
        "    plt.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egSJJvik00Iq"
      },
      "source": [
        "def train(epochs, batch_size = 64, save_interval = 200):\n",
        "  #in generator we need only X_train\n",
        "  (X_train, _), (_,_) = mnist.load_data()\n",
        "  #normalize\n",
        "  X_train = X_train /127.5 -1.\n",
        "\n",
        "  valid = np.ones((batch_size,1))\n",
        "  fakes = np.zeros((batch_size,1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    idx = np.random.randint(0,X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    #normail in 0,1 specify shape\n",
        "    #gnerate 64(batch size) images\n",
        "    noise = np.random.normal(0,1,(batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)  \n",
        "\n",
        "    #train discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs,valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    d_loss = np.add(d_loss_real, d_loss_fake) *0.5 #take mean\n",
        "\n",
        "    #Training the GAN, inverse y label\n",
        "    noise = np.random.normal(0,1 ,(batch_size, latent_dim))\n",
        "    g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "    #if is divisble\n",
        "    if(epoch % save_interval) == 0:\n",
        "      print(\"********* %d [D loss %f, acc : %.2f] [G loss : %f]\" % (epoch, d_loss[0], d_loss[1]*100, g_loss))\n",
        "      save_imgs(epoch)\n",
        "     # pass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(30_000)"
      ],
      "metadata": {
        "id": "wVQ3qXL3LG7G",
        "outputId": "d8fb81aa-f229-41a0-cb06-067aaf0f78d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* 0 [D loss 0.663974, acc : 51.56] [G loss : 0.757552]\n",
            "********* 200 [D loss 0.019457, acc : 100.00] [G loss : 4.786071]\n",
            "********* 400 [D loss 0.028195, acc : 100.00] [G loss : 5.112778]\n",
            "********* 600 [D loss 0.068067, acc : 98.44] [G loss : 4.638687]\n",
            "********* 800 [D loss 0.116361, acc : 92.19] [G loss : 5.248197]\n",
            "********* 1000 [D loss 0.225155, acc : 88.28] [G loss : 3.677407]\n",
            "********* 1200 [D loss 0.338569, acc : 84.38] [G loss : 2.956277]\n",
            "********* 1400 [D loss 0.595604, acc : 70.31] [G loss : 1.688558]\n",
            "********* 1600 [D loss 0.567321, acc : 70.31] [G loss : 1.436116]\n",
            "********* 1800 [D loss 0.709777, acc : 61.72] [G loss : 1.206875]\n",
            "********* 2000 [D loss 0.694242, acc : 60.94] [G loss : 0.871258]\n",
            "********* 2200 [D loss 0.685851, acc : 53.91] [G loss : 1.062214]\n",
            "********* 2400 [D loss 0.773422, acc : 51.56] [G loss : 0.869646]\n",
            "********* 2600 [D loss 0.762484, acc : 49.22] [G loss : 0.842606]\n",
            "********* 2800 [D loss 0.690958, acc : 58.59] [G loss : 0.854571]\n",
            "********* 3000 [D loss 0.756307, acc : 48.44] [G loss : 0.904546]\n",
            "********* 3200 [D loss 0.660296, acc : 61.72] [G loss : 0.838327]\n",
            "********* 3400 [D loss 0.748327, acc : 48.44] [G loss : 0.860971]\n",
            "********* 3600 [D loss 0.842319, acc : 39.84] [G loss : 0.759862]\n",
            "********* 3800 [D loss 0.711457, acc : 53.91] [G loss : 0.808879]\n",
            "********* 4000 [D loss 0.700056, acc : 53.91] [G loss : 0.759076]\n",
            "********* 4200 [D loss 0.731985, acc : 46.88] [G loss : 0.750860]\n",
            "********* 4400 [D loss 0.763585, acc : 40.62] [G loss : 0.758610]\n",
            "********* 4600 [D loss 0.698036, acc : 57.03] [G loss : 0.795165]\n",
            "********* 4800 [D loss 0.677641, acc : 53.91] [G loss : 0.811794]\n",
            "********* 5000 [D loss 0.720776, acc : 50.00] [G loss : 0.771926]\n",
            "********* 5200 [D loss 0.647875, acc : 61.72] [G loss : 0.893495]\n",
            "********* 5400 [D loss 0.704587, acc : 55.47] [G loss : 0.733219]\n",
            "********* 5600 [D loss 0.686835, acc : 58.59] [G loss : 0.758395]\n",
            "********* 5800 [D loss 0.681997, acc : 60.94] [G loss : 0.846106]\n",
            "********* 6000 [D loss 0.664872, acc : 59.38] [G loss : 0.716006]\n",
            "********* 6200 [D loss 0.730786, acc : 44.53] [G loss : 0.714165]\n",
            "********* 6400 [D loss 0.676551, acc : 56.25] [G loss : 0.731304]\n",
            "********* 6600 [D loss 0.665332, acc : 57.03] [G loss : 0.803266]\n",
            "********* 6800 [D loss 0.700878, acc : 53.12] [G loss : 0.747236]\n",
            "********* 7000 [D loss 0.692917, acc : 57.03] [G loss : 0.772739]\n",
            "********* 7200 [D loss 0.696689, acc : 60.94] [G loss : 0.785145]\n",
            "********* 7400 [D loss 0.650504, acc : 66.41] [G loss : 0.845858]\n",
            "********* 7600 [D loss 0.672704, acc : 58.59] [G loss : 0.786336]\n",
            "********* 7800 [D loss 0.728992, acc : 47.66] [G loss : 0.751178]\n",
            "********* 8000 [D loss 0.666925, acc : 64.84] [G loss : 0.759572]\n",
            "********* 8200 [D loss 0.727689, acc : 50.00] [G loss : 0.768262]\n",
            "********* 8400 [D loss 0.695802, acc : 55.47] [G loss : 0.720664]\n",
            "********* 8600 [D loss 0.721364, acc : 49.22] [G loss : 0.708519]\n",
            "********* 8800 [D loss 0.702314, acc : 48.44] [G loss : 0.751461]\n",
            "********* 9000 [D loss 0.743785, acc : 41.41] [G loss : 0.712212]\n",
            "********* 9200 [D loss 0.698674, acc : 55.47] [G loss : 0.769134]\n",
            "********* 9400 [D loss 0.728961, acc : 45.31] [G loss : 0.683714]\n",
            "********* 9600 [D loss 0.690473, acc : 54.69] [G loss : 0.715098]\n",
            "********* 9800 [D loss 0.703306, acc : 48.44] [G loss : 0.731099]\n",
            "********* 10000 [D loss 0.684868, acc : 51.56] [G loss : 0.716263]\n",
            "********* 10200 [D loss 0.709904, acc : 48.44] [G loss : 0.730192]\n",
            "********* 10400 [D loss 0.704384, acc : 46.88] [G loss : 0.702144]\n",
            "********* 10600 [D loss 0.695848, acc : 47.66] [G loss : 0.732647]\n",
            "********* 10800 [D loss 0.674746, acc : 57.03] [G loss : 0.716193]\n",
            "********* 11000 [D loss 0.702208, acc : 50.78] [G loss : 0.717560]\n",
            "********* 11200 [D loss 0.697777, acc : 52.34] [G loss : 0.741409]\n",
            "********* 11400 [D loss 0.681835, acc : 52.34] [G loss : 0.664318]\n",
            "********* 11600 [D loss 0.707715, acc : 46.88] [G loss : 0.733321]\n",
            "********* 11800 [D loss 0.673381, acc : 58.59] [G loss : 0.747961]\n",
            "********* 12000 [D loss 0.689025, acc : 53.91] [G loss : 0.731117]\n",
            "********* 12200 [D loss 0.698741, acc : 51.56] [G loss : 0.729370]\n",
            "********* 12400 [D loss 0.697428, acc : 51.56] [G loss : 0.720410]\n",
            "********* 12600 [D loss 0.694291, acc : 50.78] [G loss : 0.728770]\n",
            "********* 12800 [D loss 0.691128, acc : 57.03] [G loss : 0.726017]\n",
            "********* 13000 [D loss 0.685057, acc : 59.38] [G loss : 0.703149]\n",
            "********* 13200 [D loss 0.697313, acc : 53.91] [G loss : 0.730316]\n",
            "********* 13400 [D loss 0.688782, acc : 51.56] [G loss : 0.722074]\n",
            "********* 13600 [D loss 0.692162, acc : 56.25] [G loss : 0.727127]\n",
            "********* 13800 [D loss 0.700420, acc : 46.88] [G loss : 0.721819]\n",
            "********* 14000 [D loss 0.666319, acc : 60.94] [G loss : 0.730481]\n",
            "********* 14200 [D loss 0.691356, acc : 56.25] [G loss : 0.708354]\n",
            "********* 14400 [D loss 0.695256, acc : 51.56] [G loss : 0.699018]\n",
            "********* 14600 [D loss 0.676030, acc : 57.81] [G loss : 0.767427]\n",
            "********* 14800 [D loss 0.678537, acc : 55.47] [G loss : 0.737107]\n",
            "********* 15000 [D loss 0.675777, acc : 59.38] [G loss : 0.729161]\n",
            "********* 15200 [D loss 0.655391, acc : 63.28] [G loss : 0.723668]\n",
            "********* 15400 [D loss 0.675667, acc : 60.16] [G loss : 0.741380]\n",
            "********* 15600 [D loss 0.685863, acc : 51.56] [G loss : 0.740099]\n",
            "********* 15800 [D loss 0.703677, acc : 52.34] [G loss : 0.703771]\n",
            "********* 16000 [D loss 0.674087, acc : 59.38] [G loss : 0.729560]\n",
            "********* 16200 [D loss 0.706086, acc : 48.44] [G loss : 0.693930]\n",
            "********* 16400 [D loss 0.698259, acc : 49.22] [G loss : 0.748683]\n",
            "********* 16600 [D loss 0.675833, acc : 55.47] [G loss : 0.740685]\n",
            "********* 16800 [D loss 0.693812, acc : 54.69] [G loss : 0.711574]\n",
            "********* 17000 [D loss 0.677833, acc : 59.38] [G loss : 0.764669]\n",
            "********* 17200 [D loss 0.705725, acc : 48.44] [G loss : 0.709697]\n",
            "********* 17400 [D loss 0.681055, acc : 53.12] [G loss : 0.715792]\n",
            "********* 17600 [D loss 0.696568, acc : 52.34] [G loss : 0.724782]\n",
            "********* 17800 [D loss 0.674604, acc : 60.16] [G loss : 0.758844]\n",
            "********* 18000 [D loss 0.664467, acc : 64.06] [G loss : 0.733460]\n",
            "********* 18200 [D loss 0.679319, acc : 54.69] [G loss : 0.758120]\n",
            "********* 18400 [D loss 0.684917, acc : 54.69] [G loss : 0.730674]\n",
            "********* 18600 [D loss 0.669455, acc : 59.38] [G loss : 0.727217]\n",
            "********* 18800 [D loss 0.676688, acc : 60.16] [G loss : 0.755270]\n",
            "********* 19000 [D loss 0.678071, acc : 57.81] [G loss : 0.744318]\n",
            "********* 19200 [D loss 0.682183, acc : 56.25] [G loss : 0.770384]\n",
            "********* 19400 [D loss 0.688495, acc : 57.81] [G loss : 0.722040]\n",
            "********* 19600 [D loss 0.690192, acc : 54.69] [G loss : 0.743075]\n",
            "********* 19800 [D loss 0.690895, acc : 52.34] [G loss : 0.730511]\n",
            "********* 20000 [D loss 0.661065, acc : 55.47] [G loss : 0.733684]\n",
            "********* 20200 [D loss 0.662217, acc : 52.34] [G loss : 0.708932]\n",
            "********* 20400 [D loss 0.683593, acc : 50.78] [G loss : 0.720004]\n",
            "********* 20600 [D loss 0.679706, acc : 56.25] [G loss : 0.756850]\n",
            "********* 20800 [D loss 0.677776, acc : 60.16] [G loss : 0.750836]\n",
            "********* 21000 [D loss 0.664738, acc : 60.94] [G loss : 0.740877]\n",
            "********* 21200 [D loss 0.677251, acc : 50.78] [G loss : 0.738315]\n",
            "********* 21400 [D loss 0.687767, acc : 53.91] [G loss : 0.712270]\n",
            "********* 21600 [D loss 0.704018, acc : 50.78] [G loss : 0.707403]\n",
            "********* 21800 [D loss 0.649425, acc : 67.19] [G loss : 0.745423]\n",
            "********* 22000 [D loss 0.694362, acc : 53.12] [G loss : 0.730378]\n",
            "********* 22200 [D loss 0.670320, acc : 61.72] [G loss : 0.749654]\n",
            "********* 22400 [D loss 0.689284, acc : 54.69] [G loss : 0.703421]\n",
            "********* 22600 [D loss 0.682824, acc : 56.25] [G loss : 0.732375]\n",
            "********* 22800 [D loss 0.666723, acc : 65.62] [G loss : 0.729091]\n",
            "********* 23000 [D loss 0.690407, acc : 53.91] [G loss : 0.729364]\n",
            "********* 23200 [D loss 0.665019, acc : 60.94] [G loss : 0.730537]\n",
            "********* 23400 [D loss 0.691469, acc : 55.47] [G loss : 0.726395]\n",
            "********* 23600 [D loss 0.676924, acc : 60.16] [G loss : 0.760969]\n",
            "********* 23800 [D loss 0.678820, acc : 56.25] [G loss : 0.720960]\n",
            "********* 24000 [D loss 0.671375, acc : 61.72] [G loss : 0.746538]\n",
            "********* 24200 [D loss 0.711773, acc : 44.53] [G loss : 0.727692]\n",
            "********* 24400 [D loss 0.683716, acc : 55.47] [G loss : 0.717762]\n",
            "********* 24600 [D loss 0.664982, acc : 61.72] [G loss : 0.698415]\n",
            "********* 24800 [D loss 0.686877, acc : 56.25] [G loss : 0.773455]\n",
            "********* 25000 [D loss 0.653524, acc : 64.84] [G loss : 0.741391]\n",
            "********* 25200 [D loss 0.670249, acc : 62.50] [G loss : 0.761097]\n",
            "********* 25400 [D loss 0.688502, acc : 59.38] [G loss : 0.736362]\n",
            "********* 25600 [D loss 0.674816, acc : 60.16] [G loss : 0.718357]\n",
            "********* 25800 [D loss 0.691094, acc : 50.78] [G loss : 0.747047]\n",
            "********* 26000 [D loss 0.671448, acc : 61.72] [G loss : 0.749102]\n",
            "********* 26200 [D loss 0.670608, acc : 60.94] [G loss : 0.763132]\n",
            "********* 26400 [D loss 0.671204, acc : 62.50] [G loss : 0.757449]\n",
            "********* 26600 [D loss 0.655987, acc : 63.28] [G loss : 0.761426]\n",
            "********* 26800 [D loss 0.692435, acc : 52.34] [G loss : 0.761514]\n",
            "********* 27000 [D loss 0.683953, acc : 58.59] [G loss : 0.734162]\n",
            "********* 27200 [D loss 0.676110, acc : 61.72] [G loss : 0.749288]\n",
            "********* 27400 [D loss 0.687153, acc : 53.12] [G loss : 0.715506]\n",
            "********* 27600 [D loss 0.676215, acc : 56.25] [G loss : 0.763373]\n",
            "********* 27800 [D loss 0.671893, acc : 59.38] [G loss : 0.746649]\n",
            "********* 28000 [D loss 0.673227, acc : 60.94] [G loss : 0.769307]\n",
            "********* 28200 [D loss 0.643910, acc : 65.62] [G loss : 0.756787]\n",
            "********* 28400 [D loss 0.680927, acc : 56.25] [G loss : 0.764763]\n",
            "********* 28600 [D loss 0.698159, acc : 48.44] [G loss : 0.706615]\n",
            "********* 28800 [D loss 0.677543, acc : 56.25] [G loss : 0.747655]\n",
            "********* 29000 [D loss 0.682864, acc : 58.59] [G loss : 0.726829]\n",
            "********* 29200 [D loss 0.687620, acc : 54.69] [G loss : 0.754459]\n",
            "********* 29400 [D loss 0.662121, acc : 59.38] [G loss : 0.769608]\n",
            "********* 29600 [D loss 0.682467, acc : 56.25] [G loss : 0.780808]\n",
            "********* 29800 [D loss 0.672597, acc : 57.81] [G loss : 0.781214]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a new prediction"
      ],
      "metadata": {
        "id": "XkG1bqz3Vgxp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh37uv1torG5"
      },
      "source": [
        "new_noise = np.random.normal(0,1,(1,latent_dim))\n",
        "new_gen_img = generator.predict(new_noise)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_gen_img.shape"
      ],
      "metadata": {
        "id": "cgYtzdd2VxuF",
        "outputId": "8687a4f2-9603-46c5-bcca-5f40c2548f0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(new_gen_img[0,:,:], cmap = \"gray\")"
      ],
      "metadata": {
        "id": "4z4_rXwAV9Cy",
        "outputId": "796de4ac-4137-482c-f0df-71b32ead12f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4a9d0b32d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}