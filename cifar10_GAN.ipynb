{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CifarGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k5mFBuzzl2a"
      },
      "source": [
        "from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "!mkdir generated_images"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Parameters for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RThZMDruz9cB",
        "outputId": "e24513ef-b8c1-4ab5-f6c1-76f4bb6c72c7"
      },
      "source": [
        "img_width = 32\n",
        "img_height = 32\n",
        "channels = 3\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100\n",
        "adam = Adam(lr=0.0002)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiqZpri0iQh"
      },
      "source": [
        "#we wish to have an output shape of (32,32,3) upscaling the initial 256*4*4 using Conv2DTranspose\n",
        "\n",
        "def build_generator():\n",
        "  model = Sequential([\n",
        "    Dense(256 * 4 * 4, input_dim = latent_dim),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "    Reshape((4,4,256)),\n",
        "\n",
        "    #upscale an image\n",
        "    Conv2DTranspose(128, (4,4), strides = (2,2), padding = \"same\"),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "\n",
        "    Conv2DTranspose(128, (4,4), strides = (2,2), padding = \"same\"),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "\n",
        "    Conv2DTranspose(128, (4,4), strides = (2,2), padding = \"same\"),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "\n",
        "    Conv2D(3,(3,3),activation = \"tanh\", padding = \"same\")\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2JzEAPv0lKt",
        "outputId": "09532ccd-1b62-4f59-ace3-fcfa07699c3d"
      },
      "source": [
        "def build_discriminator():\n",
        "\n",
        "  model = Sequential([\n",
        "    #input match the input data\n",
        "    Conv2D(64, (3,3), padding = \"same\", input_shape = img_shape),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "\n",
        "    Conv2D(128, (3,3), padding = \"same\", input_shape = img_shape),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "\n",
        "    Conv2D(128, (3,3), padding = \"same\", input_shape = img_shape),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "\n",
        "    Conv2D(256, (3,3), padding = \"same\", input_shape = img_shape),\n",
        "    LeakyReLU(alpha = 0.2),\n",
        "\n",
        "    #flatten alllows us to have a binary output\n",
        "    Flatten(),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    #1 outpu to classify fake from real\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "    \n",
        "  ])\n",
        "  \n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 128)       147584    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 256)       295168    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 262144)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 262144)            0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 262145    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 780,545\n",
            "Trainable params: 780,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ue3TEd0xLy"
      },
      "source": [
        "GAN = Sequential()\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.layers[1].trainable = False\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPqU8dZDaQmE",
        "outputId": "4b075d82-4aaa-407d-bf6f-49cb3e7a784c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "GAN.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 32, 32, 3)         1466115   \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 1)                 780545    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,246,660\n",
            "Trainable params: 1,466,115\n",
            "Non-trainable params: 780,545\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    r, c = 2, 2\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "    # print(\"%.8f\" % save_name)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    # gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    gen_imgs = (gen_imgs + 1) / 2.0\n",
        "    # gen_imgs = gen_imgs * 255\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    plt.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egSJJvik00Iq",
        "outputId": "a266d891-ce96-4f7f-e28c-c83baf8a46ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def train(epochs, batch_size=64, save_interval=200):\n",
        "  (X_train, _), (_, _) = cifar10.load_data()\n",
        "\n",
        "  #Rescale data between -1 and 1\n",
        "  X_train = X_train / 127.5 -1.\n",
        "  bat_per_epo = int(X_train.shape[0] / batch_size)\n",
        "  \n",
        "  #adds a dimension at th end, for the channel for gray pictures from (28,28) to (28,28,1), but for RGB we already have (28,28,3) so no neeed\n",
        "  #X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "  #Create our Y for our Neural Networks\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for j in range(bat_per_epo):\n",
        "      #Get Random Batch\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "\n",
        "      #Generate Fake Images\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "      gen_imgs = generator.predict(noise)\n",
        "\n",
        "      #Train discriminator\n",
        "      d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "      \n",
        "      #inverse y label\n",
        "      g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "      print(\"******* %d %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, j, d_loss[0], 100* d_loss[1], g_loss))\n",
        "\n",
        "    save_imgs(epoch)\n",
        "\n",
        "\n",
        "train(30000, batch_size=64, save_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n",
            "******* 0 0 [D loss: 0.691988, acc: 39.84%] [G loss: 0.684648]\n",
            "******* 0 1 [D loss: 0.475138, acc: 50.00%] [G loss: 0.675276]\n",
            "******* 0 2 [D loss: 0.400196, acc: 50.00%] [G loss: 0.663944]\n",
            "******* 0 3 [D loss: 0.380146, acc: 50.00%] [G loss: 0.668222]\n",
            "******* 0 4 [D loss: 0.372777, acc: 50.00%] [G loss: 0.703469]\n",
            "******* 0 5 [D loss: 0.340337, acc: 98.44%] [G loss: 0.792401]\n",
            "******* 0 6 [D loss: 0.284709, acc: 100.00%] [G loss: 0.972625]\n",
            "******* 0 7 [D loss: 0.206501, acc: 100.00%] [G loss: 1.292089]\n",
            "******* 0 8 [D loss: 0.145881, acc: 100.00%] [G loss: 1.718528]\n",
            "******* 0 9 [D loss: 0.103858, acc: 100.00%] [G loss: 2.118136]\n",
            "******* 0 10 [D loss: 0.223651, acc: 96.88%] [G loss: 1.966610]\n",
            "******* 0 11 [D loss: 0.204999, acc: 97.66%] [G loss: 1.678829]\n",
            "******* 0 12 [D loss: 0.150400, acc: 100.00%] [G loss: 1.535702]\n",
            "******* 0 13 [D loss: 0.187544, acc: 100.00%] [G loss: 1.506113]\n",
            "******* 0 14 [D loss: 0.220925, acc: 99.22%] [G loss: 1.692023]\n",
            "******* 0 15 [D loss: 0.136577, acc: 100.00%] [G loss: 2.410205]\n",
            "******* 0 16 [D loss: 0.047920, acc: 99.22%] [G loss: 4.166692]\n",
            "******* 0 17 [D loss: 0.053830, acc: 99.22%] [G loss: 6.588853]\n",
            "******* 0 18 [D loss: 0.002302, acc: 100.00%] [G loss: 8.703032]\n",
            "******* 0 19 [D loss: 0.002936, acc: 100.00%] [G loss: 9.490219]\n",
            "******* 0 20 [D loss: 0.135965, acc: 98.44%] [G loss: 8.545856]\n",
            "******* 0 21 [D loss: 0.060179, acc: 97.66%] [G loss: 7.031055]\n",
            "******* 0 22 [D loss: 0.001951, acc: 100.00%] [G loss: 5.910171]\n",
            "******* 0 23 [D loss: 0.047674, acc: 99.22%] [G loss: 4.876221]\n",
            "******* 0 24 [D loss: 0.005848, acc: 100.00%] [G loss: 4.419163]\n",
            "******* 0 25 [D loss: 0.007276, acc: 100.00%] [G loss: 4.248657]\n",
            "******* 0 26 [D loss: 0.007561, acc: 100.00%] [G loss: 4.245614]\n",
            "******* 0 27 [D loss: 0.007134, acc: 100.00%] [G loss: 4.326130]\n",
            "******* 0 28 [D loss: 0.006499, acc: 100.00%] [G loss: 4.441727]\n",
            "******* 0 29 [D loss: 0.005947, acc: 100.00%] [G loss: 4.533384]\n",
            "******* 0 30 [D loss: 0.006057, acc: 100.00%] [G loss: 4.526293]\n",
            "******* 0 31 [D loss: 0.008453, acc: 100.00%] [G loss: 4.238209]\n",
            "******* 0 32 [D loss: 0.025111, acc: 100.00%] [G loss: 3.430734]\n",
            "******* 0 33 [D loss: 0.118485, acc: 98.44%] [G loss: 2.866132]\n",
            "******* 0 34 [D loss: 0.168454, acc: 95.31%] [G loss: 3.677449]\n",
            "******* 0 35 [D loss: 0.017526, acc: 100.00%] [G loss: 6.393891]\n",
            "******* 0 36 [D loss: 0.000279, acc: 100.00%] [G loss: 10.634533]\n",
            "******* 0 37 [D loss: 0.000001, acc: 100.00%] [G loss: 16.062054]\n",
            "******* 0 38 [D loss: 0.188188, acc: 98.44%] [G loss: 14.543756]\n",
            "******* 0 39 [D loss: 0.139444, acc: 98.44%] [G loss: 9.904184]\n",
            "******* 0 40 [D loss: 0.005756, acc: 99.22%] [G loss: 6.658953]\n",
            "******* 0 41 [D loss: 0.004940, acc: 100.00%] [G loss: 4.498099]\n",
            "******* 0 42 [D loss: 0.029121, acc: 100.00%] [G loss: 4.206557]\n",
            "******* 0 43 [D loss: 0.032191, acc: 99.22%] [G loss: 5.401268]\n",
            "******* 0 44 [D loss: 0.003773, acc: 100.00%] [G loss: 7.062832]\n",
            "******* 0 45 [D loss: 0.000315, acc: 100.00%] [G loss: 8.519119]\n",
            "******* 0 46 [D loss: 0.000081, acc: 100.00%] [G loss: 9.725492]\n",
            "******* 0 47 [D loss: 0.000029, acc: 100.00%] [G loss: 10.583169]\n",
            "******* 0 48 [D loss: 0.003022, acc: 100.00%] [G loss: 10.754242]\n",
            "******* 0 49 [D loss: 0.033287, acc: 99.22%] [G loss: 7.389115]\n",
            "******* 0 50 [D loss: 0.002457, acc: 100.00%] [G loss: 4.719913]\n",
            "******* 0 51 [D loss: 0.063615, acc: 99.22%] [G loss: 7.062738]\n",
            "******* 0 52 [D loss: 0.000020, acc: 100.00%] [G loss: 13.506599]\n",
            "******* 0 53 [D loss: 0.000033, acc: 100.00%] [G loss: 18.610468]\n",
            "******* 0 54 [D loss: 0.005093, acc: 100.00%] [G loss: 21.379581]\n",
            "******* 0 55 [D loss: 0.000000, acc: 100.00%] [G loss: 22.470280]\n",
            "******* 0 56 [D loss: 0.304027, acc: 98.44%] [G loss: 17.559715]\n",
            "******* 0 57 [D loss: 0.000001, acc: 100.00%] [G loss: 12.352764]\n",
            "******* 0 58 [D loss: 0.002722, acc: 100.00%] [G loss: 6.438545]\n",
            "******* 0 59 [D loss: 2.074488, acc: 50.78%] [G loss: 15.340384]\n",
            "******* 0 60 [D loss: 0.284014, acc: 98.44%] [G loss: 31.697495]\n",
            "******* 0 61 [D loss: 0.124179, acc: 99.22%] [G loss: 41.694149]\n",
            "******* 0 62 [D loss: 0.000002, acc: 100.00%] [G loss: 46.767456]\n",
            "******* 0 63 [D loss: 0.000000, acc: 100.00%] [G loss: 49.859741]\n",
            "******* 0 64 [D loss: 0.486177, acc: 98.44%] [G loss: 49.642242]\n",
            "******* 0 65 [D loss: 0.266277, acc: 99.22%] [G loss: 48.137779]\n",
            "******* 0 66 [D loss: 0.109292, acc: 99.22%] [G loss: 45.270355]\n",
            "******* 0 67 [D loss: 0.190772, acc: 99.22%] [G loss: 41.396111]\n",
            "******* 0 68 [D loss: 0.166210, acc: 98.44%] [G loss: 35.555801]\n",
            "******* 0 69 [D loss: 0.036984, acc: 99.22%] [G loss: 30.694885]\n",
            "******* 0 70 [D loss: 0.153626, acc: 99.22%] [G loss: 20.640774]\n",
            "******* 0 71 [D loss: 19.259251, acc: 50.00%] [G loss: 34.121712]\n",
            "******* 0 72 [D loss: 0.061285, acc: 97.66%] [G loss: 80.782822]\n",
            "******* 0 73 [D loss: 0.111334, acc: 99.22%] [G loss: 144.669373]\n",
            "******* 0 74 [D loss: 22.064720, acc: 51.56%] [G loss: 0.000575]\n",
            "******* 0 75 [D loss: 112.055742, acc: 48.44%] [G loss: 0.000000]\n",
            "******* 0 76 [D loss: 39.425073, acc: 49.22%] [G loss: 90.195915]\n",
            "******* 0 77 [D loss: 16.080526, acc: 57.81%] [G loss: 281.255737]\n",
            "******* 0 78 [D loss: 45.862717, acc: 53.91%] [G loss: 272.710175]\n",
            "******* 0 79 [D loss: 48.900997, acc: 55.47%] [G loss: 157.595932]\n",
            "******* 0 80 [D loss: 35.303205, acc: 53.12%] [G loss: 60.020447]\n",
            "******* 0 81 [D loss: 16.080192, acc: 54.69%] [G loss: 6.555265]\n",
            "******* 0 82 [D loss: 5.517845, acc: 20.31%] [G loss: 0.011111]\n",
            "******* 0 83 [D loss: 2.315707, acc: 50.00%] [G loss: 0.022619]\n",
            "******* 0 84 [D loss: 1.684387, acc: 50.00%] [G loss: 0.097401]\n",
            "******* 0 85 [D loss: 0.966918, acc: 50.00%] [G loss: 0.436013]\n",
            "******* 0 86 [D loss: 0.316613, acc: 81.25%] [G loss: 1.739620]\n",
            "******* 0 87 [D loss: 0.114222, acc: 99.22%] [G loss: 2.418556]\n",
            "******* 0 88 [D loss: 0.093366, acc: 100.00%] [G loss: 2.483625]\n",
            "******* 0 89 [D loss: 0.427419, acc: 79.69%] [G loss: 1.783031]\n",
            "******* 0 90 [D loss: 1.141120, acc: 66.41%] [G loss: 2.483537]\n",
            "******* 0 91 [D loss: 1.355189, acc: 66.41%] [G loss: 5.547965]\n",
            "******* 0 92 [D loss: 1.891444, acc: 72.66%] [G loss: 10.364362]\n",
            "******* 0 93 [D loss: 2.812516, acc: 67.97%] [G loss: 15.541521]\n",
            "******* 0 94 [D loss: 5.241903, acc: 57.81%] [G loss: 18.075785]\n",
            "******* 0 95 [D loss: 6.519560, acc: 55.47%] [G loss: 18.953741]\n",
            "******* 0 96 [D loss: 3.560138, acc: 66.41%] [G loss: 23.222916]\n",
            "******* 0 97 [D loss: 1.620342, acc: 80.47%] [G loss: 26.170237]\n",
            "******* 0 98 [D loss: 0.874209, acc: 85.94%] [G loss: 23.368456]\n",
            "******* 0 99 [D loss: 0.183018, acc: 95.31%] [G loss: 20.812706]\n",
            "******* 0 100 [D loss: 0.083886, acc: 98.44%] [G loss: 15.748260]\n",
            "******* 0 101 [D loss: 0.309704, acc: 97.66%] [G loss: 9.929926]\n",
            "******* 0 102 [D loss: 0.622823, acc: 77.34%] [G loss: 20.608667]\n",
            "******* 0 103 [D loss: 0.188720, acc: 97.66%] [G loss: 43.739456]\n",
            "******* 0 104 [D loss: 0.346920, acc: 95.31%] [G loss: 63.459808]\n",
            "******* 0 105 [D loss: 0.485463, acc: 96.09%] [G loss: 76.511482]\n",
            "******* 0 106 [D loss: 0.700063, acc: 92.97%] [G loss: 77.229355]\n",
            "******* 0 107 [D loss: 1.090730, acc: 91.41%] [G loss: 77.801468]\n",
            "******* 0 108 [D loss: 0.421963, acc: 94.53%] [G loss: 67.600258]\n",
            "******* 0 109 [D loss: 0.531055, acc: 95.31%] [G loss: 45.900612]\n",
            "******* 0 110 [D loss: 0.801613, acc: 91.41%] [G loss: 22.809586]\n",
            "******* 0 111 [D loss: 0.314294, acc: 93.75%] [G loss: 7.068748]\n",
            "******* 0 112 [D loss: 0.604098, acc: 50.78%] [G loss: 8.794355]\n",
            "******* 0 113 [D loss: 0.231143, acc: 93.75%] [G loss: 19.056232]\n",
            "******* 0 114 [D loss: 0.162356, acc: 96.88%] [G loss: 23.028728]\n",
            "******* 0 115 [D loss: 0.311087, acc: 96.09%] [G loss: 20.892513]\n",
            "******* 0 116 [D loss: 0.167483, acc: 95.31%] [G loss: 18.118477]\n",
            "******* 0 117 [D loss: 0.140059, acc: 97.66%] [G loss: 12.283449]\n",
            "******* 0 118 [D loss: 0.230716, acc: 96.09%] [G loss: 8.185578]\n",
            "******* 0 119 [D loss: 0.374765, acc: 96.09%] [G loss: 4.703682]\n",
            "******* 0 120 [D loss: 0.874877, acc: 57.03%] [G loss: 3.578898]\n",
            "******* 0 121 [D loss: 0.153981, acc: 98.44%] [G loss: 7.197375]\n",
            "******* 0 122 [D loss: 0.119175, acc: 97.66%] [G loss: 8.361103]\n",
            "******* 0 123 [D loss: 0.010953, acc: 99.22%] [G loss: 10.001072]\n",
            "******* 0 124 [D loss: 0.051962, acc: 99.22%] [G loss: 10.140494]\n",
            "******* 0 125 [D loss: 0.003452, acc: 100.00%] [G loss: 11.359915]\n",
            "******* 0 126 [D loss: 0.003380, acc: 100.00%] [G loss: 12.483063]\n",
            "******* 0 127 [D loss: 0.002380, acc: 100.00%] [G loss: 10.897624]\n",
            "******* 0 128 [D loss: 0.013690, acc: 99.22%] [G loss: 11.638271]\n",
            "******* 0 129 [D loss: 0.227907, acc: 98.44%] [G loss: 10.536060]\n",
            "******* 0 130 [D loss: 0.000881, acc: 100.00%] [G loss: 8.446445]\n",
            "******* 0 131 [D loss: 0.090322, acc: 98.44%] [G loss: 8.641784]\n",
            "******* 0 132 [D loss: 0.065720, acc: 99.22%] [G loss: 8.070997]\n",
            "******* 0 133 [D loss: 0.013612, acc: 99.22%] [G loss: 7.427346]\n",
            "******* 0 134 [D loss: 0.039749, acc: 99.22%] [G loss: 6.422066]\n",
            "******* 0 135 [D loss: 0.020457, acc: 99.22%] [G loss: 5.095235]\n",
            "******* 0 136 [D loss: 0.168342, acc: 94.53%] [G loss: 6.859741]\n",
            "******* 0 137 [D loss: 0.043167, acc: 99.22%] [G loss: 9.104763]\n",
            "******* 0 138 [D loss: 0.001966, acc: 100.00%] [G loss: 11.987629]\n",
            "******* 0 139 [D loss: 0.000299, acc: 100.00%] [G loss: 14.343637]\n",
            "******* 0 140 [D loss: 0.002375, acc: 100.00%] [G loss: 17.268864]\n",
            "******* 0 141 [D loss: 0.063127, acc: 99.22%] [G loss: 18.258207]\n",
            "******* 0 142 [D loss: 0.000131, acc: 100.00%] [G loss: 19.393627]\n",
            "******* 0 143 [D loss: 0.000141, acc: 100.00%] [G loss: 19.107033]\n",
            "******* 0 144 [D loss: 0.000046, acc: 100.00%] [G loss: 19.655653]\n",
            "******* 0 145 [D loss: 0.000359, acc: 100.00%] [G loss: 19.181662]\n",
            "******* 0 146 [D loss: 0.000621, acc: 100.00%] [G loss: 18.417557]\n",
            "******* 0 147 [D loss: 0.050501, acc: 98.44%] [G loss: 18.618572]\n",
            "******* 0 148 [D loss: 0.000846, acc: 100.00%] [G loss: 17.765900]\n",
            "******* 0 149 [D loss: 0.000259, acc: 100.00%] [G loss: 18.966831]\n",
            "******* 0 150 [D loss: 0.017836, acc: 99.22%] [G loss: 17.503946]\n",
            "******* 0 151 [D loss: 0.002640, acc: 100.00%] [G loss: 17.959198]\n",
            "******* 0 152 [D loss: 0.011379, acc: 99.22%] [G loss: 17.908884]\n",
            "******* 0 153 [D loss: 0.005964, acc: 100.00%] [G loss: 17.020271]\n",
            "******* 0 154 [D loss: 0.048123, acc: 98.44%] [G loss: 16.824097]\n",
            "******* 0 155 [D loss: 0.001500, acc: 100.00%] [G loss: 16.229946]\n",
            "******* 0 156 [D loss: 0.000264, acc: 100.00%] [G loss: 15.971851]\n",
            "******* 0 157 [D loss: 0.007158, acc: 100.00%] [G loss: 14.996815]\n",
            "******* 0 158 [D loss: 0.091823, acc: 98.44%] [G loss: 14.758455]\n",
            "******* 0 159 [D loss: 0.093900, acc: 99.22%] [G loss: 13.547362]\n",
            "******* 0 160 [D loss: 0.040255, acc: 99.22%] [G loss: 12.195571]\n",
            "******* 0 161 [D loss: 0.008122, acc: 100.00%] [G loss: 11.348621]\n",
            "******* 0 162 [D loss: 0.057434, acc: 97.66%] [G loss: 10.179512]\n",
            "******* 0 163 [D loss: 0.019534, acc: 99.22%] [G loss: 8.891136]\n",
            "******* 0 164 [D loss: 0.002968, acc: 100.00%] [G loss: 7.933105]\n",
            "******* 0 165 [D loss: 0.005631, acc: 100.00%] [G loss: 7.816291]\n",
            "******* 0 166 [D loss: 0.004000, acc: 100.00%] [G loss: 7.216371]\n",
            "******* 0 167 [D loss: 0.019964, acc: 99.22%] [G loss: 7.184275]\n",
            "******* 0 168 [D loss: 0.003868, acc: 100.00%] [G loss: 6.610816]\n",
            "******* 0 169 [D loss: 0.004249, acc: 100.00%] [G loss: 6.846458]\n",
            "******* 0 170 [D loss: 0.004844, acc: 100.00%] [G loss: 6.722914]\n",
            "******* 0 171 [D loss: 0.004393, acc: 100.00%] [G loss: 7.472994]\n",
            "******* 0 172 [D loss: 0.001550, acc: 100.00%] [G loss: 7.545256]\n",
            "******* 0 173 [D loss: 0.004075, acc: 100.00%] [G loss: 7.463465]\n",
            "******* 0 174 [D loss: 0.031486, acc: 99.22%] [G loss: 7.743735]\n",
            "******* 0 175 [D loss: 0.007076, acc: 100.00%] [G loss: 8.082257]\n",
            "******* 0 176 [D loss: 0.000936, acc: 100.00%] [G loss: 7.877867]\n",
            "******* 0 177 [D loss: 0.032823, acc: 99.22%] [G loss: 8.765907]\n",
            "******* 0 178 [D loss: 0.004669, acc: 100.00%] [G loss: 8.434556]\n",
            "******* 0 179 [D loss: 0.075496, acc: 99.22%] [G loss: 10.069923]\n",
            "******* 0 180 [D loss: 0.004892, acc: 100.00%] [G loss: 10.397758]\n",
            "******* 0 181 [D loss: 0.014765, acc: 100.00%] [G loss: 11.455851]\n",
            "******* 0 182 [D loss: 0.034640, acc: 98.44%] [G loss: 11.531389]\n",
            "******* 0 183 [D loss: 0.079525, acc: 98.44%] [G loss: 11.265429]\n",
            "******* 0 184 [D loss: 0.008571, acc: 100.00%] [G loss: 10.638620]\n",
            "******* 0 185 [D loss: 0.005469, acc: 100.00%] [G loss: 10.578067]\n",
            "******* 0 186 [D loss: 0.003376, acc: 100.00%] [G loss: 11.373412]\n",
            "******* 0 187 [D loss: 0.006722, acc: 100.00%] [G loss: 10.565086]\n",
            "******* 0 188 [D loss: 0.006197, acc: 100.00%] [G loss: 10.743773]\n",
            "******* 0 189 [D loss: 0.000143, acc: 100.00%] [G loss: 10.157471]\n",
            "******* 0 190 [D loss: 0.005567, acc: 100.00%] [G loss: 10.843111]\n",
            "******* 0 191 [D loss: 0.005899, acc: 100.00%] [G loss: 11.184866]\n",
            "******* 0 192 [D loss: 0.034032, acc: 99.22%] [G loss: 10.920225]\n",
            "******* 0 193 [D loss: 0.005525, acc: 100.00%] [G loss: 10.868810]\n",
            "******* 0 194 [D loss: 0.002121, acc: 100.00%] [G loss: 10.926069]\n",
            "******* 0 195 [D loss: 0.016218, acc: 99.22%] [G loss: 11.177135]\n",
            "******* 0 196 [D loss: 0.003245, acc: 100.00%] [G loss: 11.450506]\n",
            "******* 0 197 [D loss: 0.001350, acc: 100.00%] [G loss: 11.241936]\n",
            "******* 0 198 [D loss: 0.005146, acc: 100.00%] [G loss: 10.613592]\n",
            "******* 0 199 [D loss: 0.002081, acc: 100.00%] [G loss: 11.092546]\n",
            "******* 0 200 [D loss: 0.001647, acc: 100.00%] [G loss: 11.649199]\n",
            "******* 0 201 [D loss: 0.030298, acc: 99.22%] [G loss: 10.884495]\n",
            "******* 0 202 [D loss: 0.000850, acc: 100.00%] [G loss: 10.807823]\n",
            "******* 0 203 [D loss: 0.002068, acc: 100.00%] [G loss: 11.492990]\n",
            "******* 0 204 [D loss: 0.000641, acc: 100.00%] [G loss: 11.365381]\n",
            "******* 0 205 [D loss: 0.002450, acc: 100.00%] [G loss: 12.059400]\n",
            "******* 0 206 [D loss: 0.002651, acc: 100.00%] [G loss: 11.853027]\n",
            "******* 0 207 [D loss: 0.001692, acc: 100.00%] [G loss: 12.092628]\n",
            "******* 0 208 [D loss: 0.001376, acc: 100.00%] [G loss: 12.240543]\n",
            "******* 0 209 [D loss: 0.008415, acc: 99.22%] [G loss: 12.280757]\n",
            "******* 0 210 [D loss: 0.001682, acc: 100.00%] [G loss: 12.154865]\n",
            "******* 0 211 [D loss: 0.001062, acc: 100.00%] [G loss: 12.006621]\n",
            "******* 0 212 [D loss: 0.002538, acc: 100.00%] [G loss: 12.182723]\n",
            "******* 0 213 [D loss: 0.070589, acc: 98.44%] [G loss: 11.307816]\n",
            "******* 0 214 [D loss: 0.002403, acc: 100.00%] [G loss: 12.808312]\n",
            "******* 0 215 [D loss: 0.014211, acc: 99.22%] [G loss: 12.480771]\n",
            "******* 0 216 [D loss: 0.017477, acc: 98.44%] [G loss: 11.193768]\n",
            "******* 0 217 [D loss: 0.001213, acc: 100.00%] [G loss: 11.933241]\n",
            "******* 0 218 [D loss: 0.007673, acc: 100.00%] [G loss: 10.572580]\n",
            "******* 0 219 [D loss: 0.009075, acc: 100.00%] [G loss: 11.257624]\n",
            "******* 0 220 [D loss: 0.005961, acc: 100.00%] [G loss: 11.998236]\n",
            "******* 0 221 [D loss: 0.006904, acc: 100.00%] [G loss: 12.045605]\n",
            "******* 0 222 [D loss: 0.002784, acc: 100.00%] [G loss: 12.246284]\n",
            "******* 0 223 [D loss: 0.005466, acc: 100.00%] [G loss: 12.530609]\n",
            "******* 0 224 [D loss: 0.012940, acc: 100.00%] [G loss: 12.717439]\n",
            "******* 0 225 [D loss: 0.079216, acc: 99.22%] [G loss: 14.016818]\n",
            "******* 0 226 [D loss: 0.002900, acc: 100.00%] [G loss: 12.857298]\n",
            "******* 0 227 [D loss: 0.002877, acc: 100.00%] [G loss: 14.960475]\n",
            "******* 0 228 [D loss: 0.103872, acc: 97.66%] [G loss: 13.519657]\n",
            "******* 0 229 [D loss: 0.001598, acc: 100.00%] [G loss: 13.556756]\n",
            "******* 0 230 [D loss: 0.016768, acc: 99.22%] [G loss: 13.384572]\n",
            "******* 0 231 [D loss: 0.013731, acc: 99.22%] [G loss: 12.590194]\n",
            "******* 0 232 [D loss: 0.007720, acc: 100.00%] [G loss: 11.977014]\n",
            "******* 0 233 [D loss: 0.003056, acc: 100.00%] [G loss: 12.632341]\n",
            "******* 0 234 [D loss: 0.010331, acc: 100.00%] [G loss: 11.747261]\n",
            "******* 0 235 [D loss: 0.063136, acc: 99.22%] [G loss: 12.834268]\n",
            "******* 0 236 [D loss: 0.008288, acc: 100.00%] [G loss: 12.893565]\n",
            "******* 0 237 [D loss: 0.013538, acc: 99.22%] [G loss: 11.900496]\n",
            "******* 0 238 [D loss: 0.009677, acc: 100.00%] [G loss: 13.175140]\n",
            "******* 0 239 [D loss: 0.003489, acc: 100.00%] [G loss: 11.229900]\n",
            "******* 0 240 [D loss: 0.065924, acc: 98.44%] [G loss: 11.893772]\n",
            "******* 0 241 [D loss: 0.010912, acc: 100.00%] [G loss: 10.607401]\n",
            "******* 0 242 [D loss: 0.007686, acc: 100.00%] [G loss: 11.761093]\n",
            "******* 0 243 [D loss: 0.005132, acc: 100.00%] [G loss: 11.783049]\n",
            "******* 0 244 [D loss: 0.003892, acc: 100.00%] [G loss: 10.689037]\n",
            "******* 0 245 [D loss: 0.026339, acc: 99.22%] [G loss: 12.218473]\n",
            "******* 0 246 [D loss: 0.004027, acc: 100.00%] [G loss: 11.190561]\n",
            "******* 0 247 [D loss: 0.023114, acc: 99.22%] [G loss: 12.164220]\n",
            "******* 0 248 [D loss: 0.005278, acc: 100.00%] [G loss: 11.589136]\n",
            "******* 0 249 [D loss: 0.025371, acc: 99.22%] [G loss: 13.369670]\n",
            "******* 0 250 [D loss: 0.012894, acc: 100.00%] [G loss: 13.770444]\n",
            "******* 0 251 [D loss: 0.080465, acc: 99.22%] [G loss: 13.520044]\n",
            "******* 0 252 [D loss: 0.014331, acc: 99.22%] [G loss: 14.352787]\n",
            "******* 0 253 [D loss: 0.071000, acc: 98.44%] [G loss: 14.898175]\n",
            "******* 0 254 [D loss: 0.005094, acc: 100.00%] [G loss: 13.449450]\n",
            "******* 0 255 [D loss: 0.011360, acc: 100.00%] [G loss: 14.018781]\n",
            "******* 0 256 [D loss: 0.005445, acc: 100.00%] [G loss: 13.988045]\n",
            "******* 0 257 [D loss: 0.039579, acc: 99.22%] [G loss: 14.087396]\n",
            "******* 0 258 [D loss: 0.002882, acc: 100.00%] [G loss: 13.400761]\n",
            "******* 0 259 [D loss: 0.001755, acc: 100.00%] [G loss: 13.629313]\n",
            "******* 0 260 [D loss: 0.001385, acc: 100.00%] [G loss: 13.441168]\n",
            "******* 0 261 [D loss: 0.208679, acc: 97.66%] [G loss: 13.657486]\n",
            "******* 0 262 [D loss: 0.049914, acc: 99.22%] [G loss: 14.114021]\n",
            "******* 0 263 [D loss: 0.005604, acc: 100.00%] [G loss: 14.829903]\n",
            "******* 0 264 [D loss: 0.033277, acc: 97.66%] [G loss: 14.624153]\n",
            "******* 0 265 [D loss: 0.027135, acc: 99.22%] [G loss: 15.483329]\n",
            "******* 0 266 [D loss: 0.019043, acc: 98.44%] [G loss: 14.114036]\n",
            "******* 0 267 [D loss: 0.059765, acc: 96.88%] [G loss: 12.919373]\n",
            "******* 0 268 [D loss: 0.030573, acc: 98.44%] [G loss: 14.467182]\n",
            "******* 0 269 [D loss: 0.019281, acc: 99.22%] [G loss: 15.914139]\n",
            "******* 0 270 [D loss: 0.003561, acc: 100.00%] [G loss: 13.478789]\n",
            "******* 0 271 [D loss: 0.004106, acc: 100.00%] [G loss: 13.951639]\n",
            "******* 0 272 [D loss: 0.102538, acc: 98.44%] [G loss: 13.895078]\n",
            "******* 0 273 [D loss: 0.153370, acc: 98.44%] [G loss: 14.479905]\n",
            "******* 0 274 [D loss: 0.027236, acc: 98.44%] [G loss: 14.545986]\n",
            "******* 0 275 [D loss: 0.198386, acc: 98.44%] [G loss: 17.352581]\n",
            "******* 0 276 [D loss: 0.035984, acc: 98.44%] [G loss: 17.531090]\n",
            "******* 0 277 [D loss: 0.036518, acc: 98.44%] [G loss: 19.972204]\n",
            "******* 0 278 [D loss: 0.027270, acc: 98.44%] [G loss: 21.472563]\n",
            "******* 0 279 [D loss: 0.059794, acc: 98.44%] [G loss: 19.528553]\n",
            "******* 0 280 [D loss: 0.085955, acc: 96.88%] [G loss: 19.516499]\n",
            "******* 0 281 [D loss: 0.056037, acc: 97.66%] [G loss: 16.710842]\n",
            "******* 0 282 [D loss: 0.126014, acc: 94.53%] [G loss: 15.826724]\n",
            "******* 0 283 [D loss: 0.134579, acc: 97.66%] [G loss: 12.555552]\n",
            "******* 0 284 [D loss: 0.076664, acc: 98.44%] [G loss: 8.812401]\n",
            "******* 0 285 [D loss: 0.075560, acc: 98.44%] [G loss: 8.820080]\n",
            "******* 0 286 [D loss: 0.085732, acc: 95.31%] [G loss: 11.247696]\n",
            "******* 0 287 [D loss: 0.083158, acc: 99.22%] [G loss: 12.242303]\n",
            "******* 0 288 [D loss: 0.107004, acc: 97.66%] [G loss: 11.023752]\n",
            "******* 0 289 [D loss: 1.366597, acc: 57.81%] [G loss: 28.764828]\n",
            "******* 0 290 [D loss: 0.217153, acc: 94.53%] [G loss: 63.214630]\n",
            "******* 0 291 [D loss: 4.238911, acc: 66.41%] [G loss: 79.698624]\n",
            "******* 0 292 [D loss: 5.414093, acc: 66.41%] [G loss: 73.216080]\n",
            "******* 0 293 [D loss: 7.003014, acc: 67.97%] [G loss: 60.739731]\n",
            "******* 0 294 [D loss: 4.768110, acc: 71.09%] [G loss: 37.326893]\n",
            "******* 0 295 [D loss: 2.068399, acc: 75.00%] [G loss: 24.777111]\n",
            "******* 0 296 [D loss: 1.947523, acc: 77.34%] [G loss: 12.906225]\n",
            "******* 0 297 [D loss: 0.781285, acc: 82.03%] [G loss: 2.051589]\n",
            "******* 0 298 [D loss: 1.710830, acc: 52.34%] [G loss: 1.630832]\n",
            "******* 0 299 [D loss: 0.553991, acc: 82.03%] [G loss: 4.429759]\n",
            "******* 0 300 [D loss: 0.870795, acc: 80.47%] [G loss: 7.150517]\n",
            "******* 0 301 [D loss: 0.553609, acc: 81.25%] [G loss: 7.220551]\n",
            "******* 0 302 [D loss: 0.523474, acc: 85.16%] [G loss: 9.031047]\n",
            "******* 0 303 [D loss: 0.533287, acc: 81.25%] [G loss: 7.914114]\n",
            "******* 0 304 [D loss: 0.658234, acc: 78.12%] [G loss: 7.722086]\n",
            "******* 0 305 [D loss: 0.920306, acc: 75.78%] [G loss: 5.605968]\n",
            "******* 0 306 [D loss: 0.809007, acc: 76.56%] [G loss: 4.451837]\n",
            "******* 0 307 [D loss: 1.314910, acc: 70.31%] [G loss: 2.067094]\n",
            "******* 0 308 [D loss: 2.106335, acc: 53.91%] [G loss: 1.984578]\n",
            "******* 0 309 [D loss: 1.097235, acc: 53.12%] [G loss: 6.537505]\n",
            "******* 0 310 [D loss: 0.223158, acc: 90.62%] [G loss: 14.338980]\n",
            "******* 0 311 [D loss: 0.487795, acc: 85.94%] [G loss: 21.588902]\n",
            "******* 0 312 [D loss: 0.255730, acc: 89.84%] [G loss: 24.430679]\n",
            "******* 0 313 [D loss: 0.234298, acc: 92.19%] [G loss: 22.882572]\n",
            "******* 0 314 [D loss: 0.245757, acc: 92.19%] [G loss: 17.249624]\n",
            "******* 0 315 [D loss: 0.123749, acc: 96.88%] [G loss: 8.589007]\n",
            "******* 0 316 [D loss: 1.792004, acc: 78.91%] [G loss: 4.836395]\n",
            "******* 0 317 [D loss: 2.196642, acc: 72.66%] [G loss: 6.905024]\n",
            "******* 0 318 [D loss: 0.778466, acc: 87.50%] [G loss: 14.874905]\n",
            "******* 0 319 [D loss: 0.955479, acc: 82.03%] [G loss: 16.124664]\n",
            "******* 0 320 [D loss: 1.763706, acc: 74.22%] [G loss: 17.977337]\n",
            "******* 0 321 [D loss: 1.714679, acc: 71.88%] [G loss: 14.526702]\n",
            "******* 0 322 [D loss: 1.367209, acc: 76.56%] [G loss: 8.932617]\n",
            "******* 0 323 [D loss: 1.373992, acc: 66.41%] [G loss: 4.844694]\n",
            "******* 0 324 [D loss: 1.091895, acc: 71.09%] [G loss: 4.503330]\n",
            "******* 0 325 [D loss: 0.559506, acc: 86.72%] [G loss: 5.900620]\n",
            "******* 0 326 [D loss: 0.616723, acc: 82.03%] [G loss: 5.959821]\n",
            "******* 0 327 [D loss: 0.840538, acc: 81.25%] [G loss: 5.344709]\n",
            "******* 0 328 [D loss: 0.764652, acc: 85.16%] [G loss: 4.705764]\n",
            "******* 0 329 [D loss: 0.478822, acc: 85.94%] [G loss: 4.741665]\n",
            "******* 0 330 [D loss: 0.516715, acc: 79.69%] [G loss: 4.828654]\n",
            "******* 0 331 [D loss: 0.382807, acc: 86.72%] [G loss: 3.721257]\n",
            "******* 0 332 [D loss: 0.536332, acc: 82.81%] [G loss: 3.393686]\n",
            "******* 0 333 [D loss: 0.289302, acc: 86.72%] [G loss: 3.671752]\n",
            "******* 0 334 [D loss: 0.299859, acc: 89.84%] [G loss: 3.834303]\n",
            "******* 0 335 [D loss: 0.404129, acc: 87.50%] [G loss: 4.336088]\n",
            "******* 0 336 [D loss: 0.261419, acc: 89.84%] [G loss: 4.862971]\n",
            "******* 0 337 [D loss: 0.207787, acc: 93.75%] [G loss: 6.078753]\n",
            "******* 0 338 [D loss: 0.246834, acc: 89.84%] [G loss: 5.729443]\n",
            "******* 0 339 [D loss: 0.395646, acc: 88.28%] [G loss: 5.322376]\n",
            "******* 0 340 [D loss: 0.310372, acc: 89.06%] [G loss: 4.632479]\n",
            "******* 0 341 [D loss: 0.338489, acc: 87.50%] [G loss: 3.391922]\n",
            "******* 0 342 [D loss: 0.389943, acc: 83.59%] [G loss: 3.423875]\n",
            "******* 0 343 [D loss: 0.261183, acc: 92.97%] [G loss: 3.617824]\n",
            "******* 0 344 [D loss: 0.267962, acc: 91.41%] [G loss: 4.330308]\n",
            "******* 0 345 [D loss: 0.110325, acc: 95.31%] [G loss: 4.937066]\n",
            "******* 0 346 [D loss: 0.272519, acc: 88.28%] [G loss: 4.599595]\n",
            "******* 0 347 [D loss: 0.326317, acc: 89.06%] [G loss: 3.886289]\n",
            "******* 0 348 [D loss: 0.234538, acc: 92.97%] [G loss: 3.625321]\n",
            "******* 0 349 [D loss: 0.201589, acc: 93.75%] [G loss: 3.284750]\n",
            "******* 0 350 [D loss: 0.176245, acc: 94.53%] [G loss: 3.524871]\n",
            "******* 0 351 [D loss: 0.184941, acc: 92.19%] [G loss: 3.815059]\n",
            "******* 0 352 [D loss: 0.200993, acc: 92.97%] [G loss: 4.208240]\n",
            "******* 0 353 [D loss: 0.143650, acc: 93.75%] [G loss: 4.240640]\n",
            "******* 0 354 [D loss: 0.163166, acc: 92.97%] [G loss: 4.392850]\n",
            "******* 0 355 [D loss: 0.192492, acc: 90.62%] [G loss: 3.279458]\n",
            "******* 0 356 [D loss: 0.180220, acc: 94.53%] [G loss: 3.712630]\n",
            "******* 0 357 [D loss: 0.117411, acc: 96.88%] [G loss: 4.154955]\n",
            "******* 0 358 [D loss: 0.131270, acc: 93.75%] [G loss: 4.443751]\n",
            "******* 0 359 [D loss: 0.263785, acc: 88.28%] [G loss: 4.722621]\n",
            "******* 0 360 [D loss: 0.200606, acc: 90.62%] [G loss: 3.876225]\n",
            "******* 0 361 [D loss: 0.147410, acc: 96.09%] [G loss: 3.585994]\n",
            "******* 0 362 [D loss: 0.167853, acc: 96.88%] [G loss: 3.151138]\n",
            "******* 0 363 [D loss: 0.208718, acc: 91.41%] [G loss: 2.918794]\n",
            "******* 0 364 [D loss: 0.174599, acc: 94.53%] [G loss: 3.125572]\n",
            "******* 0 365 [D loss: 0.148784, acc: 95.31%] [G loss: 3.895322]\n",
            "******* 0 366 [D loss: 0.128473, acc: 96.09%] [G loss: 4.221653]\n",
            "******* 0 367 [D loss: 0.164446, acc: 93.75%] [G loss: 4.303147]\n",
            "******* 0 368 [D loss: 0.200441, acc: 94.53%] [G loss: 3.950467]\n",
            "******* 0 369 [D loss: 0.196950, acc: 93.75%] [G loss: 3.703703]\n",
            "******* 0 370 [D loss: 0.139021, acc: 94.53%] [G loss: 3.582149]\n",
            "******* 0 371 [D loss: 0.117373, acc: 96.88%] [G loss: 3.900205]\n",
            "******* 0 372 [D loss: 0.177853, acc: 93.75%] [G loss: 3.727246]\n",
            "******* 0 373 [D loss: 0.156231, acc: 92.19%] [G loss: 3.789874]\n",
            "******* 0 374 [D loss: 0.194545, acc: 92.97%] [G loss: 3.993605]\n",
            "******* 0 375 [D loss: 0.089377, acc: 97.66%] [G loss: 4.324696]\n",
            "******* 0 376 [D loss: 0.154686, acc: 93.75%] [G loss: 3.885149]\n",
            "******* 0 377 [D loss: 0.176297, acc: 92.97%] [G loss: 3.237833]\n",
            "******* 0 378 [D loss: 0.111541, acc: 96.88%] [G loss: 3.027160]\n",
            "******* 0 379 [D loss: 0.121066, acc: 96.88%] [G loss: 3.284358]\n",
            "******* 0 380 [D loss: 0.111180, acc: 97.66%] [G loss: 3.679125]\n",
            "******* 0 381 [D loss: 0.134178, acc: 95.31%] [G loss: 3.628818]\n",
            "******* 0 382 [D loss: 0.108087, acc: 95.31%] [G loss: 3.970836]\n",
            "******* 0 383 [D loss: 0.190651, acc: 91.41%] [G loss: 3.318655]\n",
            "******* 0 384 [D loss: 0.141592, acc: 97.66%] [G loss: 3.912544]\n",
            "******* 0 385 [D loss: 0.127491, acc: 96.09%] [G loss: 4.288057]\n",
            "******* 0 386 [D loss: 0.109830, acc: 97.66%] [G loss: 4.692107]\n",
            "******* 0 387 [D loss: 0.124357, acc: 96.09%] [G loss: 4.350911]\n",
            "******* 0 388 [D loss: 0.083696, acc: 98.44%] [G loss: 4.498921]\n",
            "******* 0 389 [D loss: 0.063623, acc: 98.44%] [G loss: 4.477587]\n",
            "******* 0 390 [D loss: 0.087251, acc: 98.44%] [G loss: 4.464611]\n",
            "******* 0 391 [D loss: 0.084238, acc: 96.88%] [G loss: 4.300749]\n",
            "******* 0 392 [D loss: 0.047522, acc: 98.44%] [G loss: 4.144151]\n",
            "******* 0 393 [D loss: 0.096882, acc: 97.66%] [G loss: 4.451147]\n",
            "******* 0 394 [D loss: 0.075303, acc: 96.88%] [G loss: 4.786443]\n",
            "******* 0 395 [D loss: 0.058356, acc: 98.44%] [G loss: 4.670752]\n",
            "******* 0 396 [D loss: 0.049641, acc: 98.44%] [G loss: 4.909391]\n",
            "******* 0 397 [D loss: 0.081464, acc: 97.66%] [G loss: 4.556592]\n",
            "******* 0 398 [D loss: 0.083956, acc: 96.09%] [G loss: 4.641528]\n",
            "******* 0 399 [D loss: 0.046003, acc: 99.22%] [G loss: 4.454136]\n",
            "******* 0 400 [D loss: 0.052236, acc: 99.22%] [G loss: 4.717758]\n",
            "******* 0 401 [D loss: 0.067811, acc: 96.88%] [G loss: 4.258936]\n",
            "******* 0 402 [D loss: 0.047247, acc: 99.22%] [G loss: 5.482987]\n",
            "******* 0 403 [D loss: 0.040439, acc: 99.22%] [G loss: 5.128671]\n",
            "******* 0 404 [D loss: 0.054013, acc: 97.66%] [G loss: 5.871495]\n",
            "******* 0 405 [D loss: 0.063071, acc: 96.88%] [G loss: 5.507870]\n",
            "******* 0 406 [D loss: 0.071306, acc: 96.88%] [G loss: 5.111911]\n",
            "******* 0 407 [D loss: 0.036292, acc: 99.22%] [G loss: 4.846594]\n",
            "******* 0 408 [D loss: 0.041482, acc: 99.22%] [G loss: 5.548919]\n",
            "******* 0 409 [D loss: 0.043600, acc: 99.22%] [G loss: 5.516388]\n",
            "******* 0 410 [D loss: 0.025851, acc: 100.00%] [G loss: 5.770901]\n",
            "******* 0 411 [D loss: 0.009875, acc: 100.00%] [G loss: 6.451771]\n",
            "******* 0 412 [D loss: 0.020102, acc: 99.22%] [G loss: 6.388514]\n",
            "******* 0 413 [D loss: 0.027529, acc: 99.22%] [G loss: 6.074359]\n",
            "******* 0 414 [D loss: 0.052020, acc: 96.88%] [G loss: 6.345851]\n",
            "******* 0 415 [D loss: 0.028807, acc: 100.00%] [G loss: 6.219679]\n",
            "******* 0 416 [D loss: 0.075173, acc: 99.22%] [G loss: 6.326291]\n",
            "******* 0 417 [D loss: 0.049193, acc: 98.44%] [G loss: 6.527719]\n",
            "******* 0 418 [D loss: 0.052213, acc: 97.66%] [G loss: 7.390668]\n",
            "******* 0 419 [D loss: 0.046968, acc: 98.44%] [G loss: 7.665593]\n",
            "******* 0 420 [D loss: 0.039660, acc: 99.22%] [G loss: 9.690084]\n",
            "******* 0 421 [D loss: 0.033058, acc: 99.22%] [G loss: 9.108887]\n",
            "******* 0 422 [D loss: 0.025399, acc: 99.22%] [G loss: 7.048174]\n",
            "******* 0 423 [D loss: 0.041915, acc: 98.44%] [G loss: 8.981691]\n",
            "******* 0 424 [D loss: 0.013797, acc: 100.00%] [G loss: 7.780039]\n",
            "******* 0 425 [D loss: 0.052893, acc: 99.22%] [G loss: 10.224223]\n",
            "******* 0 426 [D loss: 0.055912, acc: 98.44%] [G loss: 7.690370]\n",
            "******* 0 427 [D loss: 0.021994, acc: 99.22%] [G loss: 6.525768]\n",
            "******* 0 428 [D loss: 0.017017, acc: 100.00%] [G loss: 8.927261]\n",
            "******* 0 429 [D loss: 0.045979, acc: 97.66%] [G loss: 9.677326]\n",
            "******* 0 430 [D loss: 0.037550, acc: 98.44%] [G loss: 10.735626]\n",
            "******* 0 431 [D loss: 0.011988, acc: 100.00%] [G loss: 9.701460]\n",
            "******* 0 432 [D loss: 0.029643, acc: 100.00%] [G loss: 8.405783]\n",
            "******* 0 433 [D loss: 0.018605, acc: 99.22%] [G loss: 6.895374]\n",
            "******* 0 434 [D loss: 0.007519, acc: 100.00%] [G loss: 8.514591]\n",
            "******* 0 435 [D loss: 0.011943, acc: 100.00%] [G loss: 10.639711]\n",
            "******* 0 436 [D loss: 0.027609, acc: 99.22%] [G loss: 7.087760]\n",
            "******* 0 437 [D loss: 0.008024, acc: 100.00%] [G loss: 9.562389]\n",
            "******* 0 438 [D loss: 0.020827, acc: 99.22%] [G loss: 8.905774]\n",
            "******* 0 439 [D loss: 0.026878, acc: 98.44%] [G loss: 9.104572]\n",
            "******* 0 440 [D loss: 0.038024, acc: 98.44%] [G loss: 10.667410]\n",
            "******* 0 441 [D loss: 0.011683, acc: 100.00%] [G loss: 7.537023]\n",
            "******* 0 442 [D loss: 0.019048, acc: 99.22%] [G loss: 9.732423]\n",
            "******* 0 443 [D loss: 0.019376, acc: 100.00%] [G loss: 8.722548]\n",
            "******* 0 444 [D loss: 0.018496, acc: 99.22%] [G loss: 8.750177]\n",
            "******* 0 445 [D loss: 0.018871, acc: 99.22%] [G loss: 8.277378]\n",
            "******* 0 446 [D loss: 0.021438, acc: 99.22%] [G loss: 9.611595]\n",
            "******* 0 447 [D loss: 0.017839, acc: 99.22%] [G loss: 8.894860]\n",
            "******* 0 448 [D loss: 0.002086, acc: 100.00%] [G loss: 9.775163]\n",
            "******* 0 449 [D loss: 0.046799, acc: 99.22%] [G loss: 7.924268]\n",
            "******* 0 450 [D loss: 0.015101, acc: 99.22%] [G loss: 7.761956]\n",
            "******* 0 451 [D loss: 0.014836, acc: 99.22%] [G loss: 7.495803]\n",
            "******* 0 452 [D loss: 0.013395, acc: 99.22%] [G loss: 6.686296]\n",
            "******* 0 453 [D loss: 0.015800, acc: 99.22%] [G loss: 7.724393]\n",
            "******* 0 454 [D loss: 0.032231, acc: 98.44%] [G loss: 6.677533]\n",
            "******* 0 455 [D loss: 0.049702, acc: 96.88%] [G loss: 7.003826]\n",
            "******* 0 456 [D loss: 0.023585, acc: 99.22%] [G loss: 7.877680]\n",
            "******* 0 457 [D loss: 0.005966, acc: 100.00%] [G loss: 9.229130]\n",
            "******* 0 458 [D loss: 0.039867, acc: 99.22%] [G loss: 7.263728]\n",
            "******* 0 459 [D loss: 0.058511, acc: 96.88%] [G loss: 7.764548]\n",
            "******* 0 460 [D loss: 0.004068, acc: 100.00%] [G loss: 8.701452]\n",
            "******* 0 461 [D loss: 0.004996, acc: 100.00%] [G loss: 5.954162]\n",
            "******* 0 462 [D loss: 0.006810, acc: 100.00%] [G loss: 6.992367]\n",
            "******* 0 463 [D loss: 0.015136, acc: 100.00%] [G loss: 7.749986]\n",
            "******* 0 464 [D loss: 0.017069, acc: 99.22%] [G loss: 6.959717]\n",
            "******* 0 465 [D loss: 0.010452, acc: 100.00%] [G loss: 9.134511]\n",
            "******* 0 466 [D loss: 0.024753, acc: 98.44%] [G loss: 8.752816]\n",
            "******* 0 467 [D loss: 0.002525, acc: 100.00%] [G loss: 10.380914]\n",
            "******* 0 468 [D loss: 0.052856, acc: 99.22%] [G loss: 9.273153]\n",
            "******* 0 469 [D loss: 0.020980, acc: 99.22%] [G loss: 8.779704]\n",
            "******* 0 470 [D loss: 0.015065, acc: 100.00%] [G loss: 7.393525]\n",
            "******* 0 471 [D loss: 0.026042, acc: 99.22%] [G loss: 7.915741]\n",
            "******* 0 472 [D loss: 0.028184, acc: 99.22%] [G loss: 6.754490]\n",
            "******* 0 473 [D loss: 0.011542, acc: 100.00%] [G loss: 7.548007]\n",
            "******* 0 474 [D loss: 0.023527, acc: 99.22%] [G loss: 6.161100]\n",
            "******* 0 475 [D loss: 0.015324, acc: 99.22%] [G loss: 8.561446]\n",
            "******* 0 476 [D loss: 0.034279, acc: 98.44%] [G loss: 10.077202]\n",
            "******* 0 477 [D loss: 0.023617, acc: 99.22%] [G loss: 9.003983]\n",
            "******* 0 478 [D loss: 0.010495, acc: 100.00%] [G loss: 10.011921]\n",
            "******* 0 479 [D loss: 0.021986, acc: 99.22%] [G loss: 10.361240]\n",
            "******* 0 480 [D loss: 0.023350, acc: 99.22%] [G loss: 9.953470]\n",
            "******* 0 481 [D loss: 0.009195, acc: 100.00%] [G loss: 8.430796]\n",
            "******* 0 482 [D loss: 0.025378, acc: 98.44%] [G loss: 9.199919]\n",
            "******* 0 483 [D loss: 0.009749, acc: 100.00%] [G loss: 7.828576]\n",
            "******* 0 484 [D loss: 0.079848, acc: 96.88%] [G loss: 7.383695]\n",
            "******* 0 485 [D loss: 0.062616, acc: 97.66%] [G loss: 6.806434]\n",
            "******* 0 486 [D loss: 0.018118, acc: 99.22%] [G loss: 5.738128]\n",
            "******* 0 487 [D loss: 0.016093, acc: 100.00%] [G loss: 6.108095]\n",
            "******* 0 488 [D loss: 0.025489, acc: 99.22%] [G loss: 5.552448]\n",
            "******* 0 489 [D loss: 0.027356, acc: 99.22%] [G loss: 6.007234]\n",
            "******* 0 490 [D loss: 0.030087, acc: 99.22%] [G loss: 6.299365]\n",
            "******* 0 491 [D loss: 0.051643, acc: 97.66%] [G loss: 6.503918]\n",
            "******* 0 492 [D loss: 0.039194, acc: 98.44%] [G loss: 6.589312]\n",
            "******* 0 493 [D loss: 0.052177, acc: 96.09%] [G loss: 6.758757]\n",
            "******* 0 494 [D loss: 0.023292, acc: 98.44%] [G loss: 6.773411]\n",
            "******* 0 495 [D loss: 0.060852, acc: 97.66%] [G loss: 5.646027]\n",
            "******* 0 496 [D loss: 0.024263, acc: 98.44%] [G loss: 6.152070]\n",
            "******* 0 497 [D loss: 0.028601, acc: 98.44%] [G loss: 6.306717]\n",
            "******* 0 498 [D loss: 0.024484, acc: 99.22%] [G loss: 5.732348]\n",
            "******* 0 499 [D loss: 0.032003, acc: 98.44%] [G loss: 5.869133]\n",
            "******* 0 500 [D loss: 0.014264, acc: 100.00%] [G loss: 5.596525]\n",
            "******* 0 501 [D loss: 0.101608, acc: 97.66%] [G loss: 4.997243]\n",
            "******* 0 502 [D loss: 0.037108, acc: 98.44%] [G loss: 4.711849]\n",
            "******* 0 503 [D loss: 0.024314, acc: 99.22%] [G loss: 5.714993]\n",
            "******* 0 504 [D loss: 0.069935, acc: 96.88%] [G loss: 5.749104]\n",
            "******* 0 505 [D loss: 0.005519, acc: 100.00%] [G loss: 6.243932]\n",
            "******* 0 506 [D loss: 0.047996, acc: 97.66%] [G loss: 6.837698]\n",
            "******* 0 507 [D loss: 0.039518, acc: 99.22%] [G loss: 6.953462]\n",
            "******* 0 508 [D loss: 0.025415, acc: 99.22%] [G loss: 6.924072]\n",
            "******* 0 509 [D loss: 0.031826, acc: 98.44%] [G loss: 6.708558]\n",
            "******* 0 510 [D loss: 0.019260, acc: 99.22%] [G loss: 6.107405]\n",
            "******* 0 511 [D loss: 0.093133, acc: 95.31%] [G loss: 4.696422]\n",
            "******* 0 512 [D loss: 0.039198, acc: 98.44%] [G loss: 4.928930]\n",
            "******* 0 513 [D loss: 0.079301, acc: 96.09%] [G loss: 5.523947]\n",
            "******* 0 514 [D loss: 0.054208, acc: 98.44%] [G loss: 5.617108]\n",
            "******* 0 515 [D loss: 0.079071, acc: 96.09%] [G loss: 6.241693]\n",
            "******* 0 516 [D loss: 0.031158, acc: 98.44%] [G loss: 6.147221]\n",
            "******* 0 517 [D loss: 0.021220, acc: 99.22%] [G loss: 5.697726]\n",
            "******* 0 518 [D loss: 0.088536, acc: 96.09%] [G loss: 5.604750]\n",
            "******* 0 519 [D loss: 0.060643, acc: 96.09%] [G loss: 4.950291]\n",
            "******* 0 520 [D loss: 0.059449, acc: 97.66%] [G loss: 4.718445]\n",
            "******* 0 521 [D loss: 0.062465, acc: 97.66%] [G loss: 5.218616]\n",
            "******* 0 522 [D loss: 0.041166, acc: 99.22%] [G loss: 6.020286]\n",
            "******* 0 523 [D loss: 0.111461, acc: 95.31%] [G loss: 6.052629]\n",
            "******* 0 524 [D loss: 0.083257, acc: 96.88%] [G loss: 5.654965]\n",
            "******* 0 525 [D loss: 0.149730, acc: 92.97%] [G loss: 4.256088]\n",
            "******* 0 526 [D loss: 0.155530, acc: 93.75%] [G loss: 4.263427]\n",
            "******* 0 527 [D loss: 0.111426, acc: 95.31%] [G loss: 4.377507]\n",
            "******* 0 528 [D loss: 0.140275, acc: 92.19%] [G loss: 5.347484]\n",
            "******* 0 529 [D loss: 0.090911, acc: 96.88%] [G loss: 5.271515]\n",
            "******* 0 530 [D loss: 0.034688, acc: 99.22%] [G loss: 5.767042]\n",
            "******* 0 531 [D loss: 0.073857, acc: 96.88%] [G loss: 5.694324]\n",
            "******* 0 532 [D loss: 0.105801, acc: 95.31%] [G loss: 4.938128]\n",
            "******* 0 533 [D loss: 0.087639, acc: 99.22%] [G loss: 4.292823]\n",
            "******* 0 534 [D loss: 0.119555, acc: 96.09%] [G loss: 4.680789]\n",
            "******* 0 535 [D loss: 0.136840, acc: 96.09%] [G loss: 5.026070]\n",
            "******* 0 536 [D loss: 0.076192, acc: 97.66%] [G loss: 5.523269]\n",
            "******* 0 537 [D loss: 0.052765, acc: 98.44%] [G loss: 5.722840]\n",
            "******* 0 538 [D loss: 0.042398, acc: 98.44%] [G loss: 5.645047]\n",
            "******* 0 539 [D loss: 0.064364, acc: 97.66%] [G loss: 5.791232]\n",
            "******* 0 540 [D loss: 0.085622, acc: 97.66%] [G loss: 4.406547]\n",
            "******* 0 541 [D loss: 0.099408, acc: 98.44%] [G loss: 4.480599]\n",
            "******* 0 542 [D loss: 0.101550, acc: 93.75%] [G loss: 4.148485]\n",
            "******* 0 543 [D loss: 0.033883, acc: 100.00%] [G loss: 4.845949]\n",
            "******* 0 544 [D loss: 0.044924, acc: 98.44%] [G loss: 5.348166]\n",
            "******* 0 545 [D loss: 0.092227, acc: 97.66%] [G loss: 5.490841]\n",
            "******* 0 546 [D loss: 0.039875, acc: 99.22%] [G loss: 6.030765]\n",
            "******* 0 547 [D loss: 0.056910, acc: 97.66%] [G loss: 5.343704]\n",
            "******* 0 548 [D loss: 0.050148, acc: 98.44%] [G loss: 5.215997]\n",
            "******* 0 549 [D loss: 0.030970, acc: 100.00%] [G loss: 4.977957]\n",
            "******* 0 550 [D loss: 0.045721, acc: 97.66%] [G loss: 5.492722]\n",
            "******* 0 551 [D loss: 0.040809, acc: 97.66%] [G loss: 4.820492]\n",
            "******* 0 552 [D loss: 0.039158, acc: 98.44%] [G loss: 4.022376]\n",
            "******* 0 553 [D loss: 0.034873, acc: 99.22%] [G loss: 4.288977]\n",
            "******* 0 554 [D loss: 0.043706, acc: 99.22%] [G loss: 4.981193]\n",
            "******* 0 555 [D loss: 0.106106, acc: 98.44%] [G loss: 5.924374]\n",
            "******* 0 556 [D loss: 0.047417, acc: 98.44%] [G loss: 5.210026]\n",
            "******* 0 557 [D loss: 0.041565, acc: 99.22%] [G loss: 5.474689]\n",
            "******* 0 558 [D loss: 0.076194, acc: 96.88%] [G loss: 5.060506]\n",
            "******* 0 559 [D loss: 0.059753, acc: 96.88%] [G loss: 4.362697]\n",
            "******* 0 560 [D loss: 0.046626, acc: 98.44%] [G loss: 5.168913]\n",
            "******* 0 561 [D loss: 0.062865, acc: 99.22%] [G loss: 6.381132]\n",
            "******* 0 562 [D loss: 0.079931, acc: 96.09%] [G loss: 6.333173]\n",
            "******* 0 563 [D loss: 0.103981, acc: 96.09%] [G loss: 5.403535]\n",
            "******* 0 564 [D loss: 0.105704, acc: 96.09%] [G loss: 4.518520]\n",
            "******* 0 565 [D loss: 0.138839, acc: 94.53%] [G loss: 4.569924]\n",
            "******* 0 566 [D loss: 0.081927, acc: 96.09%] [G loss: 4.682863]\n",
            "******* 0 567 [D loss: 0.090531, acc: 95.31%] [G loss: 5.270050]\n",
            "******* 0 568 [D loss: 0.122117, acc: 92.97%] [G loss: 5.746307]\n",
            "******* 0 569 [D loss: 0.174252, acc: 91.41%] [G loss: 6.650461]\n",
            "******* 0 570 [D loss: 0.205569, acc: 90.62%] [G loss: 5.324706]\n",
            "******* 0 571 [D loss: 0.127967, acc: 94.53%] [G loss: 7.044430]\n",
            "******* 0 572 [D loss: 0.225637, acc: 83.59%] [G loss: 6.376409]\n",
            "******* 0 573 [D loss: 0.125706, acc: 91.41%] [G loss: 6.645600]\n",
            "******* 0 574 [D loss: 0.143506, acc: 92.19%] [G loss: 6.205993]\n",
            "******* 0 575 [D loss: 0.199366, acc: 88.28%] [G loss: 5.284964]\n",
            "******* 0 576 [D loss: 0.126587, acc: 96.88%] [G loss: 2.892909]\n",
            "******* 0 577 [D loss: 0.129638, acc: 93.75%] [G loss: 3.171592]\n",
            "******* 0 578 [D loss: 0.063851, acc: 100.00%] [G loss: 4.835017]\n",
            "******* 0 579 [D loss: 0.074964, acc: 96.88%] [G loss: 5.923682]\n",
            "******* 0 580 [D loss: 0.200471, acc: 95.31%] [G loss: 5.957169]\n",
            "******* 0 581 [D loss: 0.103286, acc: 97.66%] [G loss: 4.738728]\n",
            "******* 0 582 [D loss: 0.086402, acc: 97.66%] [G loss: 3.452355]\n",
            "******* 0 583 [D loss: 0.128690, acc: 98.44%] [G loss: 3.883660]\n",
            "******* 0 584 [D loss: 0.110346, acc: 96.88%] [G loss: 3.483408]\n",
            "******* 0 585 [D loss: 0.059663, acc: 99.22%] [G loss: 4.294776]\n",
            "******* 0 586 [D loss: 0.064095, acc: 98.44%] [G loss: 5.068805]\n",
            "******* 0 587 [D loss: 0.085174, acc: 98.44%] [G loss: 5.277674]\n",
            "******* 0 588 [D loss: 0.050889, acc: 100.00%] [G loss: 5.034438]\n",
            "******* 0 589 [D loss: 0.061651, acc: 98.44%] [G loss: 5.473829]\n",
            "******* 0 590 [D loss: 0.111189, acc: 96.88%] [G loss: 4.350175]\n",
            "******* 0 591 [D loss: 0.068113, acc: 98.44%] [G loss: 3.176444]\n",
            "******* 0 592 [D loss: 0.054596, acc: 100.00%] [G loss: 3.196497]\n",
            "******* 0 593 [D loss: 0.041626, acc: 100.00%] [G loss: 4.265607]\n",
            "******* 0 594 [D loss: 0.107956, acc: 98.44%] [G loss: 4.491990]\n",
            "******* 0 595 [D loss: 0.072349, acc: 98.44%] [G loss: 4.304330]\n",
            "******* 0 596 [D loss: 0.098719, acc: 97.66%] [G loss: 3.178596]\n",
            "******* 0 597 [D loss: 0.111425, acc: 97.66%] [G loss: 3.834812]\n",
            "******* 0 598 [D loss: 0.036835, acc: 99.22%] [G loss: 5.498834]\n",
            "******* 0 599 [D loss: 0.050487, acc: 96.88%] [G loss: 7.064198]\n",
            "******* 0 600 [D loss: 0.148904, acc: 96.09%] [G loss: 6.304460]\n",
            "******* 0 601 [D loss: 0.081254, acc: 98.44%] [G loss: 5.316665]\n",
            "******* 0 602 [D loss: 0.120626, acc: 96.88%] [G loss: 3.665894]\n",
            "******* 0 603 [D loss: 0.084311, acc: 99.22%] [G loss: 3.163439]\n",
            "******* 0 604 [D loss: 0.036738, acc: 99.22%] [G loss: 4.784795]\n",
            "******* 0 605 [D loss: 0.041029, acc: 98.44%] [G loss: 6.122627]\n",
            "******* 0 606 [D loss: 0.084762, acc: 97.66%] [G loss: 6.599894]\n",
            "******* 0 607 [D loss: 0.131516, acc: 96.09%] [G loss: 5.913381]\n",
            "******* 0 608 [D loss: 0.077872, acc: 98.44%] [G loss: 3.875751]\n",
            "******* 0 609 [D loss: 0.059661, acc: 99.22%] [G loss: 3.357445]\n",
            "******* 0 610 [D loss: 0.086492, acc: 98.44%] [G loss: 3.460080]\n",
            "******* 0 611 [D loss: 0.051206, acc: 99.22%] [G loss: 4.213745]\n",
            "******* 0 612 [D loss: 0.045274, acc: 99.22%] [G loss: 5.005459]\n",
            "******* 0 613 [D loss: 0.119886, acc: 96.88%] [G loss: 5.521498]\n",
            "******* 0 614 [D loss: 0.086894, acc: 96.88%] [G loss: 4.846890]\n",
            "******* 0 615 [D loss: 0.046860, acc: 100.00%] [G loss: 5.665543]\n",
            "******* 0 616 [D loss: 0.148860, acc: 96.09%] [G loss: 4.181635]\n",
            "******* 0 617 [D loss: 0.052337, acc: 99.22%] [G loss: 3.466249]\n",
            "******* 0 618 [D loss: 0.085324, acc: 97.66%] [G loss: 3.525520]\n",
            "******* 0 619 [D loss: 0.067295, acc: 99.22%] [G loss: 3.753661]\n",
            "******* 0 620 [D loss: 0.092863, acc: 97.66%] [G loss: 4.183097]\n",
            "******* 0 621 [D loss: 0.082670, acc: 97.66%] [G loss: 4.259858]\n",
            "******* 0 622 [D loss: 0.108714, acc: 94.53%] [G loss: 3.384869]\n",
            "******* 0 623 [D loss: 0.118996, acc: 95.31%] [G loss: 3.512612]\n",
            "******* 0 624 [D loss: 0.094159, acc: 98.44%] [G loss: 4.060536]\n",
            "******* 0 625 [D loss: 0.071803, acc: 96.88%] [G loss: 5.251579]\n",
            "******* 0 626 [D loss: 0.099000, acc: 97.66%] [G loss: 4.581883]\n",
            "******* 0 627 [D loss: 0.055638, acc: 98.44%] [G loss: 4.481305]\n",
            "******* 0 628 [D loss: 0.164497, acc: 96.09%] [G loss: 2.922252]\n",
            "******* 0 629 [D loss: 0.127755, acc: 94.53%] [G loss: 3.253344]\n",
            "******* 0 630 [D loss: 0.089723, acc: 97.66%] [G loss: 4.555677]\n",
            "******* 0 631 [D loss: 0.081308, acc: 96.09%] [G loss: 5.490941]\n",
            "******* 0 632 [D loss: 0.047203, acc: 98.44%] [G loss: 5.463146]\n",
            "******* 0 633 [D loss: 0.105509, acc: 97.66%] [G loss: 5.971408]\n",
            "******* 0 634 [D loss: 0.093147, acc: 96.88%] [G loss: 4.871680]\n",
            "******* 0 635 [D loss: 0.088472, acc: 99.22%] [G loss: 4.079926]\n",
            "******* 0 636 [D loss: 0.112681, acc: 96.88%] [G loss: 3.400227]\n",
            "******* 0 637 [D loss: 0.064682, acc: 98.44%] [G loss: 3.508570]\n",
            "******* 0 638 [D loss: 0.098517, acc: 96.09%] [G loss: 3.928215]\n",
            "******* 0 639 [D loss: 0.040588, acc: 99.22%] [G loss: 5.134840]\n",
            "******* 0 640 [D loss: 0.065509, acc: 96.88%] [G loss: 5.337462]\n",
            "******* 0 641 [D loss: 0.033636, acc: 99.22%] [G loss: 6.002361]\n",
            "******* 0 642 [D loss: 0.090958, acc: 94.53%] [G loss: 4.514148]\n",
            "******* 0 643 [D loss: 0.056007, acc: 99.22%] [G loss: 3.596755]\n",
            "******* 0 644 [D loss: 0.069376, acc: 98.44%] [G loss: 4.393341]\n",
            "******* 0 645 [D loss: 0.082015, acc: 96.88%] [G loss: 4.722414]\n",
            "******* 0 646 [D loss: 0.088159, acc: 97.66%] [G loss: 4.727118]\n",
            "******* 0 647 [D loss: 0.058155, acc: 98.44%] [G loss: 5.430244]\n",
            "******* 0 648 [D loss: 0.086764, acc: 97.66%] [G loss: 6.125900]\n",
            "******* 0 649 [D loss: 0.127907, acc: 92.97%] [G loss: 6.103026]\n",
            "******* 0 650 [D loss: 0.131047, acc: 94.53%] [G loss: 6.208363]\n",
            "******* 0 651 [D loss: 0.103757, acc: 97.66%] [G loss: 4.693322]\n",
            "******* 0 652 [D loss: 0.115715, acc: 96.88%] [G loss: 5.448809]\n",
            "******* 0 653 [D loss: 0.181491, acc: 93.75%] [G loss: 5.041427]\n",
            "******* 0 654 [D loss: 0.127338, acc: 92.97%] [G loss: 5.327317]\n",
            "******* 0 655 [D loss: 0.293727, acc: 86.72%] [G loss: 6.296308]\n",
            "******* 0 656 [D loss: 0.209608, acc: 91.41%] [G loss: 6.422184]\n",
            "******* 0 657 [D loss: 0.292151, acc: 89.84%] [G loss: 4.838521]\n",
            "******* 0 658 [D loss: 0.509304, acc: 81.25%] [G loss: 2.823560]\n",
            "******* 0 659 [D loss: 0.319779, acc: 86.72%] [G loss: 5.795361]\n",
            "******* 0 660 [D loss: 0.506034, acc: 83.59%] [G loss: 8.335195]\n",
            "******* 0 661 [D loss: 0.543247, acc: 84.38%] [G loss: 11.056865]\n",
            "******* 0 662 [D loss: 0.394434, acc: 85.94%] [G loss: 7.998087]\n",
            "******* 0 663 [D loss: 0.275850, acc: 89.06%] [G loss: 10.176006]\n",
            "******* 0 664 [D loss: 0.280406, acc: 89.84%] [G loss: 9.074964]\n",
            "******* 0 665 [D loss: 0.241465, acc: 89.06%] [G loss: 8.891838]\n",
            "******* 0 666 [D loss: 0.159709, acc: 94.53%] [G loss: 7.377887]\n",
            "******* 0 667 [D loss: 0.874503, acc: 78.12%] [G loss: 6.787863]\n",
            "******* 0 668 [D loss: 0.843229, acc: 72.66%] [G loss: 7.341454]\n",
            "******* 0 669 [D loss: 0.416431, acc: 88.28%] [G loss: 10.337421]\n",
            "******* 0 670 [D loss: 1.208610, acc: 72.66%] [G loss: 10.499720]\n",
            "******* 0 671 [D loss: 0.731834, acc: 81.25%] [G loss: 9.526728]\n",
            "******* 0 672 [D loss: 0.760430, acc: 81.25%] [G loss: 7.738982]\n",
            "******* 0 673 [D loss: 0.453908, acc: 86.72%] [G loss: 4.774026]\n",
            "******* 0 674 [D loss: 0.361310, acc: 87.50%] [G loss: 2.541735]\n",
            "******* 0 675 [D loss: 0.249173, acc: 93.75%] [G loss: 2.321930]\n",
            "******* 0 676 [D loss: 0.118400, acc: 96.88%] [G loss: 4.043467]\n",
            "******* 0 677 [D loss: 0.060046, acc: 97.66%] [G loss: 5.522852]\n",
            "******* 0 678 [D loss: 0.070507, acc: 96.88%] [G loss: 6.386594]\n",
            "******* 0 679 [D loss: 0.102925, acc: 94.53%] [G loss: 6.668092]\n",
            "******* 0 680 [D loss: 0.062960, acc: 96.88%] [G loss: 6.028430]\n",
            "******* 0 681 [D loss: 0.065635, acc: 98.44%] [G loss: 5.569376]\n",
            "******* 0 682 [D loss: 0.076103, acc: 96.88%] [G loss: 5.238745]\n",
            "******* 0 683 [D loss: 0.032202, acc: 100.00%] [G loss: 4.915297]\n",
            "******* 0 684 [D loss: 0.024874, acc: 100.00%] [G loss: 4.767966]\n",
            "******* 0 685 [D loss: 0.055259, acc: 96.88%] [G loss: 4.424638]\n",
            "******* 0 686 [D loss: 0.067721, acc: 97.66%] [G loss: 4.778424]\n",
            "******* 0 687 [D loss: 0.027698, acc: 99.22%] [G loss: 5.662903]\n",
            "******* 0 688 [D loss: 0.042955, acc: 98.44%] [G loss: 6.288253]\n",
            "******* 0 689 [D loss: 0.049152, acc: 98.44%] [G loss: 6.351878]\n",
            "******* 0 690 [D loss: 0.055355, acc: 98.44%] [G loss: 6.761898]\n",
            "******* 0 691 [D loss: 0.023271, acc: 100.00%] [G loss: 6.695058]\n",
            "******* 0 692 [D loss: 0.024709, acc: 99.22%] [G loss: 6.985899]\n",
            "******* 0 693 [D loss: 0.065953, acc: 98.44%] [G loss: 6.516381]\n",
            "******* 0 694 [D loss: 0.035675, acc: 98.44%] [G loss: 6.458124]\n",
            "******* 0 695 [D loss: 0.023614, acc: 99.22%] [G loss: 6.002959]\n",
            "******* 0 696 [D loss: 0.027318, acc: 100.00%] [G loss: 6.726153]\n",
            "******* 0 697 [D loss: 0.023236, acc: 100.00%] [G loss: 6.076570]\n",
            "******* 0 698 [D loss: 0.056276, acc: 98.44%] [G loss: 6.310720]\n",
            "******* 0 699 [D loss: 0.037540, acc: 98.44%] [G loss: 6.028559]\n",
            "******* 0 700 [D loss: 0.054354, acc: 97.66%] [G loss: 6.115558]\n",
            "******* 0 701 [D loss: 0.024376, acc: 100.00%] [G loss: 6.039005]\n",
            "******* 0 702 [D loss: 0.030137, acc: 99.22%] [G loss: 6.107400]\n",
            "******* 0 703 [D loss: 0.042301, acc: 98.44%] [G loss: 6.108472]\n",
            "******* 0 704 [D loss: 0.045006, acc: 98.44%] [G loss: 5.915648]\n",
            "******* 0 705 [D loss: 0.049201, acc: 98.44%] [G loss: 6.586911]\n",
            "******* 0 706 [D loss: 0.036511, acc: 98.44%] [G loss: 6.382658]\n",
            "******* 0 707 [D loss: 0.073044, acc: 96.88%] [G loss: 6.283519]\n",
            "******* 0 708 [D loss: 0.064113, acc: 97.66%] [G loss: 6.708972]\n",
            "******* 0 709 [D loss: 0.042470, acc: 98.44%] [G loss: 6.411637]\n",
            "******* 0 710 [D loss: 0.031521, acc: 99.22%] [G loss: 6.526906]\n",
            "******* 0 711 [D loss: 0.029720, acc: 99.22%] [G loss: 6.335931]\n",
            "******* 0 712 [D loss: 0.027009, acc: 100.00%] [G loss: 6.607798]\n",
            "******* 0 713 [D loss: 0.037733, acc: 98.44%] [G loss: 6.225718]\n",
            "******* 0 714 [D loss: 0.030934, acc: 99.22%] [G loss: 6.327260]\n",
            "******* 0 715 [D loss: 0.113732, acc: 96.88%] [G loss: 6.225107]\n",
            "******* 0 716 [D loss: 0.031162, acc: 99.22%] [G loss: 5.840734]\n",
            "******* 0 717 [D loss: 0.050141, acc: 97.66%] [G loss: 5.797967]\n",
            "******* 0 718 [D loss: 0.024554, acc: 100.00%] [G loss: 5.770196]\n",
            "******* 0 719 [D loss: 0.063093, acc: 98.44%] [G loss: 6.279254]\n",
            "******* 0 720 [D loss: 0.073896, acc: 96.88%] [G loss: 5.396425]\n",
            "******* 0 721 [D loss: 0.043128, acc: 98.44%] [G loss: 6.139194]\n",
            "******* 0 722 [D loss: 0.043542, acc: 98.44%] [G loss: 5.888246]\n",
            "******* 0 723 [D loss: 0.036940, acc: 99.22%] [G loss: 5.336698]\n",
            "******* 0 724 [D loss: 0.054819, acc: 98.44%] [G loss: 5.806452]\n",
            "******* 0 725 [D loss: 0.073112, acc: 96.09%] [G loss: 5.976046]\n",
            "******* 0 726 [D loss: 0.022117, acc: 100.00%] [G loss: 4.901740]\n",
            "******* 0 727 [D loss: 0.070501, acc: 96.09%] [G loss: 4.643880]\n",
            "******* 0 728 [D loss: 0.044551, acc: 99.22%] [G loss: 5.082379]\n",
            "******* 0 729 [D loss: 0.084593, acc: 96.88%] [G loss: 4.607038]\n",
            "******* 0 730 [D loss: 0.102016, acc: 96.09%] [G loss: 4.544999]\n",
            "******* 0 731 [D loss: 0.093418, acc: 95.31%] [G loss: 5.066305]\n",
            "******* 0 732 [D loss: 0.074490, acc: 96.88%] [G loss: 6.720227]\n",
            "******* 0 733 [D loss: 0.155399, acc: 90.62%] [G loss: 6.883765]\n",
            "******* 0 734 [D loss: 0.082192, acc: 96.09%] [G loss: 7.654490]\n",
            "******* 0 735 [D loss: 0.162106, acc: 92.97%] [G loss: 6.877524]\n",
            "******* 0 736 [D loss: 0.093127, acc: 96.09%] [G loss: 7.082164]\n",
            "******* 0 737 [D loss: 0.098587, acc: 95.31%] [G loss: 5.990997]\n",
            "******* 0 738 [D loss: 0.192470, acc: 88.28%] [G loss: 3.960057]\n",
            "******* 0 739 [D loss: 0.208171, acc: 92.19%] [G loss: 3.029563]\n",
            "******* 0 740 [D loss: 0.099230, acc: 98.44%] [G loss: 4.269286]\n",
            "******* 0 741 [D loss: 0.102469, acc: 96.09%] [G loss: 5.620321]\n",
            "******* 0 742 [D loss: 0.146614, acc: 95.31%] [G loss: 5.712019]\n",
            "******* 0 743 [D loss: 0.138784, acc: 93.75%] [G loss: 4.964789]\n",
            "******* 0 744 [D loss: 0.082066, acc: 97.66%] [G loss: 4.125377]\n",
            "******* 0 745 [D loss: 0.135634, acc: 95.31%] [G loss: 3.782934]\n",
            "******* 0 746 [D loss: 0.081528, acc: 97.66%] [G loss: 4.569279]\n",
            "******* 0 747 [D loss: 0.093216, acc: 96.88%] [G loss: 4.877156]\n",
            "******* 0 748 [D loss: 0.200791, acc: 93.75%] [G loss: 4.969973]\n",
            "******* 0 749 [D loss: 0.084230, acc: 97.66%] [G loss: 4.224158]\n",
            "******* 0 750 [D loss: 0.042235, acc: 99.22%] [G loss: 4.607805]\n",
            "******* 0 751 [D loss: 0.167402, acc: 93.75%] [G loss: 3.949892]\n",
            "******* 0 752 [D loss: 0.114071, acc: 95.31%] [G loss: 4.113452]\n",
            "******* 0 753 [D loss: 0.068795, acc: 96.09%] [G loss: 4.972153]\n",
            "******* 0 754 [D loss: 0.075162, acc: 96.09%] [G loss: 5.165796]\n",
            "******* 0 755 [D loss: 0.049446, acc: 96.88%] [G loss: 5.521875]\n",
            "******* 0 756 [D loss: 0.070293, acc: 96.88%] [G loss: 5.476101]\n",
            "******* 0 757 [D loss: 0.059118, acc: 97.66%] [G loss: 5.866602]\n",
            "******* 0 758 [D loss: 0.055454, acc: 97.66%] [G loss: 6.038570]\n",
            "******* 0 759 [D loss: 0.183270, acc: 96.09%] [G loss: 5.012100]\n",
            "******* 0 760 [D loss: 0.121054, acc: 96.09%] [G loss: 4.655963]\n",
            "******* 0 761 [D loss: 0.159981, acc: 93.75%] [G loss: 4.798209]\n",
            "******* 0 762 [D loss: 0.093782, acc: 96.09%] [G loss: 4.430241]\n",
            "******* 0 763 [D loss: 0.151698, acc: 94.53%] [G loss: 5.106967]\n",
            "******* 0 764 [D loss: 0.195952, acc: 93.75%] [G loss: 5.275445]\n",
            "******* 0 765 [D loss: 0.050952, acc: 98.44%] [G loss: 5.594945]\n",
            "******* 0 766 [D loss: 0.211964, acc: 91.41%] [G loss: 5.572385]\n",
            "******* 0 767 [D loss: 0.104670, acc: 95.31%] [G loss: 4.360000]\n",
            "******* 0 768 [D loss: 0.146690, acc: 96.09%] [G loss: 3.952686]\n",
            "******* 0 769 [D loss: 0.160948, acc: 94.53%] [G loss: 3.658679]\n",
            "******* 0 770 [D loss: 0.114355, acc: 94.53%] [G loss: 3.877721]\n",
            "******* 0 771 [D loss: 0.056445, acc: 99.22%] [G loss: 4.341103]\n",
            "******* 0 772 [D loss: 0.131342, acc: 96.09%] [G loss: 4.284280]\n",
            "******* 0 773 [D loss: 0.066396, acc: 97.66%] [G loss: 4.643718]\n",
            "******* 0 774 [D loss: 0.173588, acc: 94.53%] [G loss: 4.353762]\n",
            "******* 0 775 [D loss: 0.133798, acc: 96.88%] [G loss: 4.170136]\n",
            "******* 0 776 [D loss: 0.152147, acc: 96.09%] [G loss: 3.821709]\n",
            "******* 0 777 [D loss: 0.125916, acc: 93.75%] [G loss: 3.487871]\n",
            "******* 0 778 [D loss: 0.090429, acc: 98.44%] [G loss: 4.103260]\n",
            "******* 0 779 [D loss: 0.144744, acc: 94.53%] [G loss: 3.604749]\n",
            "******* 0 780 [D loss: 0.158839, acc: 96.09%] [G loss: 3.442742]\n",
            "******* 1 0 [D loss: 0.224948, acc: 91.41%] [G loss: 3.551608]\n",
            "******* 1 1 [D loss: 0.157314, acc: 95.31%] [G loss: 3.547821]\n",
            "******* 1 2 [D loss: 0.134950, acc: 95.31%] [G loss: 4.408189]\n",
            "******* 1 3 [D loss: 0.084614, acc: 97.66%] [G loss: 5.114491]\n",
            "******* 1 4 [D loss: 0.128073, acc: 95.31%] [G loss: 4.813594]\n",
            "******* 1 5 [D loss: 0.172261, acc: 92.97%] [G loss: 4.408057]\n",
            "******* 1 6 [D loss: 0.192456, acc: 92.19%] [G loss: 3.918502]\n",
            "******* 1 7 [D loss: 0.162328, acc: 92.97%] [G loss: 3.531805]\n",
            "******* 1 8 [D loss: 0.167630, acc: 93.75%] [G loss: 5.316454]\n",
            "******* 1 9 [D loss: 0.114099, acc: 96.88%] [G loss: 5.944721]\n",
            "******* 1 10 [D loss: 0.080613, acc: 98.44%] [G loss: 5.714859]\n",
            "******* 1 11 [D loss: 0.178418, acc: 91.41%] [G loss: 5.667181]\n",
            "******* 1 12 [D loss: 0.175445, acc: 91.41%] [G loss: 4.132831]\n",
            "******* 1 13 [D loss: 0.190294, acc: 92.19%] [G loss: 3.445444]\n",
            "******* 1 14 [D loss: 0.183365, acc: 91.41%] [G loss: 3.982301]\n",
            "******* 1 15 [D loss: 0.196255, acc: 92.19%] [G loss: 5.239075]\n",
            "******* 1 16 [D loss: 0.124203, acc: 96.09%] [G loss: 6.405580]\n",
            "******* 1 17 [D loss: 0.126932, acc: 95.31%] [G loss: 6.857417]\n",
            "******* 1 18 [D loss: 0.227410, acc: 90.62%] [G loss: 5.889900]\n",
            "******* 1 19 [D loss: 0.178389, acc: 90.62%] [G loss: 4.124321]\n",
            "******* 1 20 [D loss: 0.207207, acc: 91.41%] [G loss: 3.779446]\n",
            "******* 1 21 [D loss: 0.201877, acc: 93.75%] [G loss: 3.859524]\n",
            "******* 1 22 [D loss: 0.293742, acc: 90.62%] [G loss: 4.726404]\n",
            "******* 1 23 [D loss: 0.140318, acc: 94.53%] [G loss: 5.582233]\n",
            "******* 1 24 [D loss: 0.145473, acc: 93.75%] [G loss: 7.019377]\n",
            "******* 1 25 [D loss: 0.140252, acc: 92.97%] [G loss: 5.632960]\n",
            "******* 1 26 [D loss: 0.121125, acc: 97.66%] [G loss: 5.262374]\n",
            "******* 1 27 [D loss: 0.202894, acc: 92.19%] [G loss: 4.449545]\n",
            "******* 1 28 [D loss: 0.240186, acc: 87.50%] [G loss: 3.365040]\n",
            "******* 1 29 [D loss: 0.182756, acc: 92.19%] [G loss: 4.045609]\n",
            "******* 1 30 [D loss: 0.121644, acc: 96.09%] [G loss: 3.636031]\n",
            "******* 1 31 [D loss: 0.116411, acc: 97.66%] [G loss: 4.999197]\n",
            "******* 1 32 [D loss: 0.151295, acc: 95.31%] [G loss: 4.245690]\n",
            "******* 1 33 [D loss: 0.138830, acc: 94.53%] [G loss: 3.691740]\n",
            "******* 1 34 [D loss: 0.123967, acc: 96.88%] [G loss: 4.071527]\n",
            "******* 1 35 [D loss: 0.123550, acc: 99.22%] [G loss: 3.274814]\n",
            "******* 1 36 [D loss: 0.143125, acc: 96.09%] [G loss: 3.188616]\n",
            "******* 1 37 [D loss: 0.158718, acc: 97.66%] [G loss: 3.333135]\n",
            "******* 1 38 [D loss: 0.129082, acc: 96.88%] [G loss: 3.444227]\n",
            "******* 1 39 [D loss: 0.145249, acc: 93.75%] [G loss: 3.699172]\n",
            "******* 1 40 [D loss: 0.168586, acc: 96.88%] [G loss: 3.112640]\n",
            "******* 1 41 [D loss: 0.161923, acc: 96.88%] [G loss: 3.217521]\n",
            "******* 1 42 [D loss: 0.182277, acc: 92.19%] [G loss: 3.366085]\n",
            "******* 1 43 [D loss: 0.214242, acc: 93.75%] [G loss: 3.067351]\n",
            "******* 1 44 [D loss: 0.168370, acc: 96.09%] [G loss: 3.224680]\n",
            "******* 1 45 [D loss: 0.161055, acc: 96.09%] [G loss: 3.254169]\n",
            "******* 1 46 [D loss: 0.348591, acc: 89.84%] [G loss: 2.769503]\n",
            "******* 1 47 [D loss: 0.311219, acc: 84.38%] [G loss: 3.186955]\n",
            "******* 1 48 [D loss: 0.302395, acc: 88.28%] [G loss: 3.738423]\n",
            "******* 1 49 [D loss: 0.332944, acc: 85.94%] [G loss: 3.137837]\n",
            "******* 1 50 [D loss: 0.400842, acc: 87.50%] [G loss: 1.994876]\n",
            "******* 1 51 [D loss: 0.403348, acc: 82.81%] [G loss: 2.307652]\n",
            "******* 1 52 [D loss: 0.195975, acc: 92.97%] [G loss: 3.643954]\n",
            "******* 1 53 [D loss: 0.267718, acc: 90.62%] [G loss: 3.861175]\n",
            "******* 1 54 [D loss: 0.157248, acc: 94.53%] [G loss: 3.666298]\n",
            "******* 1 55 [D loss: 0.177083, acc: 94.53%] [G loss: 3.382583]\n",
            "******* 1 56 [D loss: 0.141589, acc: 93.75%] [G loss: 3.199732]\n",
            "******* 1 57 [D loss: 0.095561, acc: 96.09%] [G loss: 3.047711]\n",
            "******* 1 58 [D loss: 0.087662, acc: 98.44%] [G loss: 3.498333]\n",
            "******* 1 59 [D loss: 0.104948, acc: 96.88%] [G loss: 3.954510]\n",
            "******* 1 60 [D loss: 0.084078, acc: 96.09%] [G loss: 4.182525]\n",
            "******* 1 61 [D loss: 0.086675, acc: 98.44%] [G loss: 4.057697]\n",
            "******* 1 62 [D loss: 0.105322, acc: 95.31%] [G loss: 3.319836]\n",
            "******* 1 63 [D loss: 0.125831, acc: 97.66%] [G loss: 3.177519]\n",
            "******* 1 64 [D loss: 0.086602, acc: 97.66%] [G loss: 3.799414]\n",
            "******* 1 65 [D loss: 0.062144, acc: 99.22%] [G loss: 4.941977]\n",
            "******* 1 66 [D loss: 0.071528, acc: 97.66%] [G loss: 4.902099]\n",
            "******* 1 67 [D loss: 0.152250, acc: 93.75%] [G loss: 3.392926]\n",
            "******* 1 68 [D loss: 0.166052, acc: 96.88%] [G loss: 2.600051]\n",
            "******* 1 69 [D loss: 0.094222, acc: 100.00%] [G loss: 3.602926]\n",
            "******* 1 70 [D loss: 0.080876, acc: 98.44%] [G loss: 4.434862]\n",
            "******* 1 71 [D loss: 0.156293, acc: 93.75%] [G loss: 3.530281]\n",
            "******* 1 72 [D loss: 0.113214, acc: 98.44%] [G loss: 2.851203]\n",
            "******* 1 73 [D loss: 0.146652, acc: 96.09%] [G loss: 2.912677]\n",
            "******* 1 74 [D loss: 0.089981, acc: 98.44%] [G loss: 3.769514]\n",
            "******* 1 75 [D loss: 0.103362, acc: 97.66%] [G loss: 3.774471]\n",
            "******* 1 76 [D loss: 0.104791, acc: 96.09%] [G loss: 3.427675]\n",
            "******* 1 77 [D loss: 0.123298, acc: 97.66%] [G loss: 2.688569]\n",
            "******* 1 78 [D loss: 0.119931, acc: 97.66%] [G loss: 2.942822]\n",
            "******* 1 79 [D loss: 0.094815, acc: 98.44%] [G loss: 3.634957]\n",
            "******* 1 80 [D loss: 0.125718, acc: 95.31%] [G loss: 3.618780]\n",
            "******* 1 81 [D loss: 0.066205, acc: 98.44%] [G loss: 3.941043]\n",
            "******* 1 82 [D loss: 0.112015, acc: 96.88%] [G loss: 3.122209]\n",
            "******* 1 83 [D loss: 0.086176, acc: 98.44%] [G loss: 2.970891]\n",
            "******* 1 84 [D loss: 0.060399, acc: 100.00%] [G loss: 3.679831]\n",
            "******* 1 85 [D loss: 0.055368, acc: 99.22%] [G loss: 4.042299]\n",
            "******* 1 86 [D loss: 0.080225, acc: 96.88%] [G loss: 4.119420]\n",
            "******* 1 87 [D loss: 0.084293, acc: 96.88%] [G loss: 3.875307]\n",
            "******* 1 88 [D loss: 0.097944, acc: 97.66%] [G loss: 3.622285]\n",
            "******* 1 89 [D loss: 0.126340, acc: 96.09%] [G loss: 3.439868]\n",
            "******* 1 90 [D loss: 0.088899, acc: 98.44%] [G loss: 3.211376]\n",
            "******* 1 91 [D loss: 0.092849, acc: 98.44%] [G loss: 3.747005]\n",
            "******* 1 92 [D loss: 0.140984, acc: 96.88%] [G loss: 3.504439]\n",
            "******* 1 93 [D loss: 0.177183, acc: 96.09%] [G loss: 2.998832]\n",
            "******* 1 94 [D loss: 0.274587, acc: 92.19%] [G loss: 2.977680]\n",
            "******* 1 95 [D loss: 0.302636, acc: 89.06%] [G loss: 3.707373]\n",
            "******* 1 96 [D loss: 0.543570, acc: 77.34%] [G loss: 2.790568]\n",
            "******* 1 97 [D loss: 0.296258, acc: 85.94%] [G loss: 3.434614]\n",
            "******* 1 98 [D loss: 0.307683, acc: 86.72%] [G loss: 4.483759]\n",
            "******* 1 99 [D loss: 0.359555, acc: 86.72%] [G loss: 4.545023]\n",
            "******* 1 100 [D loss: 0.351980, acc: 83.59%] [G loss: 5.467155]\n",
            "******* 1 101 [D loss: 0.132770, acc: 95.31%] [G loss: 5.920250]\n",
            "******* 1 102 [D loss: 0.176808, acc: 94.53%] [G loss: 7.803562]\n",
            "******* 1 103 [D loss: 0.344000, acc: 84.38%] [G loss: 6.357070]\n",
            "******* 1 104 [D loss: 0.207138, acc: 92.97%] [G loss: 7.554204]\n",
            "******* 1 105 [D loss: 0.217688, acc: 89.84%] [G loss: 7.950685]\n",
            "******* 1 106 [D loss: 0.260215, acc: 93.75%] [G loss: 5.359494]\n",
            "******* 1 107 [D loss: 0.193042, acc: 96.09%] [G loss: 6.176258]\n",
            "******* 1 108 [D loss: 0.173540, acc: 95.31%] [G loss: 8.470737]\n",
            "******* 1 109 [D loss: 0.131149, acc: 93.75%] [G loss: 11.054875]\n",
            "******* 1 110 [D loss: 0.346960, acc: 88.28%] [G loss: 8.511671]\n",
            "******* 1 111 [D loss: 0.406849, acc: 89.06%] [G loss: 5.019774]\n",
            "******* 1 112 [D loss: 0.308667, acc: 86.72%] [G loss: 13.785482]\n",
            "******* 1 113 [D loss: 0.448983, acc: 85.16%] [G loss: 25.323471]\n",
            "******* 1 114 [D loss: 1.113805, acc: 82.03%] [G loss: 17.335819]\n",
            "******* 1 115 [D loss: 0.539766, acc: 85.16%] [G loss: 9.305761]\n",
            "******* 1 116 [D loss: 0.223929, acc: 94.53%] [G loss: 3.930916]\n",
            "******* 1 117 [D loss: 1.111214, acc: 71.09%] [G loss: 14.060139]\n",
            "******* 1 118 [D loss: 0.367133, acc: 92.19%] [G loss: 30.133060]\n",
            "******* 1 119 [D loss: 1.366179, acc: 81.25%] [G loss: 36.046345]\n",
            "******* 1 120 [D loss: 1.299189, acc: 79.69%] [G loss: 28.742786]\n",
            "******* 1 121 [D loss: 0.970592, acc: 81.25%] [G loss: 24.497963]\n",
            "******* 1 122 [D loss: 0.846941, acc: 84.38%] [G loss: 13.325943]\n",
            "******* 1 123 [D loss: 0.316112, acc: 92.97%] [G loss: 9.374781]\n",
            "******* 1 124 [D loss: 0.136592, acc: 95.31%] [G loss: 8.037819]\n",
            "******* 1 125 [D loss: 0.626127, acc: 78.12%] [G loss: 7.261915]\n",
            "******* 1 126 [D loss: 0.234410, acc: 92.97%] [G loss: 7.193289]\n",
            "******* 1 127 [D loss: 0.063586, acc: 98.44%] [G loss: 8.745626]\n",
            "******* 1 128 [D loss: 0.147356, acc: 96.09%] [G loss: 8.257530]\n",
            "******* 1 129 [D loss: 0.048236, acc: 99.22%] [G loss: 8.755171]\n",
            "******* 1 130 [D loss: 0.194070, acc: 92.97%] [G loss: 9.717890]\n",
            "******* 1 131 [D loss: 0.089231, acc: 95.31%] [G loss: 9.244810]\n",
            "******* 1 132 [D loss: 0.128926, acc: 95.31%] [G loss: 10.677576]\n",
            "******* 1 133 [D loss: 0.062854, acc: 98.44%] [G loss: 10.735179]\n",
            "******* 1 134 [D loss: 0.243211, acc: 94.53%] [G loss: 10.663357]\n",
            "******* 1 135 [D loss: 0.109343, acc: 93.75%] [G loss: 8.806820]\n",
            "******* 1 136 [D loss: 0.115673, acc: 95.31%] [G loss: 8.059937]\n",
            "******* 1 137 [D loss: 0.043213, acc: 98.44%] [G loss: 6.800070]\n",
            "******* 1 138 [D loss: 0.256640, acc: 89.84%] [G loss: 5.411314]\n",
            "******* 1 139 [D loss: 0.091300, acc: 96.09%] [G loss: 4.865073]\n",
            "******* 1 140 [D loss: 0.075340, acc: 96.09%] [G loss: 5.672925]\n",
            "******* 1 141 [D loss: 0.155137, acc: 94.53%] [G loss: 5.350368]\n",
            "******* 1 142 [D loss: 0.105566, acc: 95.31%] [G loss: 6.432132]\n",
            "******* 1 143 [D loss: 0.061983, acc: 97.66%] [G loss: 7.258973]\n",
            "******* 1 144 [D loss: 0.074367, acc: 96.09%] [G loss: 7.240633]\n",
            "******* 1 145 [D loss: 0.176469, acc: 91.41%] [G loss: 7.266275]\n",
            "******* 1 146 [D loss: 0.101314, acc: 95.31%] [G loss: 7.171025]\n",
            "******* 1 147 [D loss: 0.197639, acc: 92.19%] [G loss: 6.587833]\n",
            "******* 1 148 [D loss: 0.065501, acc: 97.66%] [G loss: 5.937541]\n",
            "******* 1 149 [D loss: 0.178743, acc: 92.97%] [G loss: 6.756114]\n",
            "******* 1 150 [D loss: 0.234590, acc: 89.84%] [G loss: 5.476975]\n",
            "******* 1 151 [D loss: 0.130209, acc: 94.53%] [G loss: 4.622316]\n",
            "******* 1 152 [D loss: 0.211438, acc: 92.19%] [G loss: 3.910823]\n",
            "******* 1 153 [D loss: 0.114715, acc: 95.31%] [G loss: 4.166368]\n",
            "******* 1 154 [D loss: 0.191297, acc: 93.75%] [G loss: 4.527170]\n",
            "******* 1 155 [D loss: 0.106807, acc: 96.88%] [G loss: 5.325534]\n",
            "******* 1 156 [D loss: 0.186432, acc: 95.31%] [G loss: 4.326185]\n",
            "******* 1 157 [D loss: 0.218801, acc: 93.75%] [G loss: 3.464021]\n",
            "******* 1 158 [D loss: 0.255867, acc: 88.28%] [G loss: 3.320085]\n",
            "******* 1 159 [D loss: 0.269468, acc: 88.28%] [G loss: 2.637892]\n",
            "******* 1 160 [D loss: 0.222021, acc: 87.50%] [G loss: 3.297250]\n",
            "******* 1 161 [D loss: 0.314443, acc: 86.72%] [G loss: 3.230140]\n",
            "******* 1 162 [D loss: 0.414545, acc: 85.94%] [G loss: 3.021444]\n",
            "******* 1 163 [D loss: 0.568398, acc: 77.34%] [G loss: 2.107748]\n",
            "******* 1 164 [D loss: 0.390679, acc: 82.81%] [G loss: 2.218038]\n",
            "******* 1 165 [D loss: 0.256985, acc: 89.84%] [G loss: 3.913078]\n",
            "******* 1 166 [D loss: 0.196342, acc: 94.53%] [G loss: 5.276309]\n",
            "******* 1 167 [D loss: 0.315775, acc: 89.06%] [G loss: 5.519114]\n",
            "******* 1 168 [D loss: 0.344580, acc: 89.84%] [G loss: 5.162570]\n",
            "******* 1 169 [D loss: 0.166879, acc: 94.53%] [G loss: 4.724047]\n",
            "******* 1 170 [D loss: 0.103478, acc: 96.88%] [G loss: 4.821955]\n",
            "******* 1 171 [D loss: 0.094430, acc: 97.66%] [G loss: 5.321761]\n",
            "******* 1 172 [D loss: 0.122139, acc: 93.75%] [G loss: 5.600260]\n",
            "******* 1 173 [D loss: 0.116029, acc: 93.75%] [G loss: 5.734940]\n",
            "******* 1 174 [D loss: 0.063250, acc: 99.22%] [G loss: 5.194320]\n",
            "******* 1 175 [D loss: 0.047766, acc: 99.22%] [G loss: 4.771556]\n",
            "******* 1 176 [D loss: 0.067120, acc: 97.66%] [G loss: 7.074421]\n",
            "******* 1 177 [D loss: 0.082452, acc: 98.44%] [G loss: 5.862597]\n",
            "******* 1 178 [D loss: 0.054048, acc: 98.44%] [G loss: 5.847645]\n",
            "******* 1 179 [D loss: 0.050509, acc: 98.44%] [G loss: 5.677629]\n",
            "******* 1 180 [D loss: 0.067201, acc: 98.44%] [G loss: 5.719808]\n",
            "******* 1 181 [D loss: 0.063011, acc: 98.44%] [G loss: 5.161197]\n",
            "******* 1 182 [D loss: 0.080648, acc: 98.44%] [G loss: 4.618968]\n",
            "******* 1 183 [D loss: 0.077592, acc: 99.22%] [G loss: 3.698161]\n",
            "******* 1 184 [D loss: 0.059765, acc: 99.22%] [G loss: 3.559812]\n",
            "******* 1 185 [D loss: 0.084422, acc: 98.44%] [G loss: 3.979690]\n",
            "******* 1 186 [D loss: 0.078396, acc: 98.44%] [G loss: 3.404229]\n",
            "******* 1 187 [D loss: 0.101740, acc: 97.66%] [G loss: 3.657151]\n",
            "******* 1 188 [D loss: 0.201338, acc: 90.62%] [G loss: 3.259819]\n",
            "******* 1 189 [D loss: 0.254386, acc: 88.28%] [G loss: 3.390097]\n",
            "******* 1 190 [D loss: 0.277441, acc: 89.06%] [G loss: 3.798437]\n",
            "******* 1 191 [D loss: 0.368348, acc: 87.50%] [G loss: 3.942493]\n",
            "******* 1 192 [D loss: 0.304447, acc: 89.84%] [G loss: 4.013638]\n",
            "******* 1 193 [D loss: 0.302551, acc: 88.28%] [G loss: 3.412537]\n",
            "******* 1 194 [D loss: 0.350329, acc: 85.16%] [G loss: 3.473224]\n",
            "******* 1 195 [D loss: 0.428633, acc: 82.03%] [G loss: 3.221703]\n",
            "******* 1 196 [D loss: 0.390236, acc: 88.28%] [G loss: 3.491783]\n",
            "******* 1 197 [D loss: 0.325358, acc: 85.16%] [G loss: 3.683752]\n",
            "******* 1 198 [D loss: 0.325172, acc: 88.28%] [G loss: 2.960787]\n",
            "******* 1 199 [D loss: 0.458035, acc: 84.38%] [G loss: 2.841625]\n",
            "******* 1 200 [D loss: 0.219252, acc: 92.19%] [G loss: 3.754600]\n",
            "******* 1 201 [D loss: 0.138173, acc: 94.53%] [G loss: 4.755420]\n",
            "******* 1 202 [D loss: 0.152482, acc: 92.97%] [G loss: 4.473290]\n",
            "******* 1 203 [D loss: 0.111282, acc: 98.44%] [G loss: 3.666317]\n",
            "******* 1 204 [D loss: 0.148839, acc: 94.53%] [G loss: 3.951911]\n",
            "******* 1 205 [D loss: 0.111880, acc: 96.88%] [G loss: 5.401057]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS9wDLeRLUOB"
      },
      "source": [
        "noise = np.random.normal(0, 1, (1,latent_dim))\n",
        "gen_imgs = generator.predict(noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rlgu8g9Lik9"
      },
      "source": [
        "gen_imgs = (gen_imgs + 1) / 2.0\n",
        "plt.imshow(gen_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "# def display_image(epoch_no):\n",
        "#   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}